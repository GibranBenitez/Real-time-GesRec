Namespace(annotation_path='/misc/dl001/bega/Real-time-GesRec/annotation_nvGesture/nvall_but_None.json', arch='resnext-101', batch_size=8, begin_epoch=1, checkpoint=1, crop_position_in_test='c', dampening=0.9, dataset='nv', ft_begin_index=0, initial_scale=1.0, learning_rate=0.01, lr_patience=10, lr_steps=[10, 25, 50, 80, 100], manual_seed=1, mean=[114.7748, 107.7354, 99.475], mean_dataset='activitynet', modality='Depth', model='resnext', model_depth=101, momentum=0.9, n_classes=25, n_epochs=100, n_finetune_classes=25, n_scales=5, n_threads=16, n_val_samples=1, nesterov=False, no_cuda=False, no_hflip=False, no_mean_norm=False, no_softmax_in_test=False, no_train=False, no_val=False, norm_value=1, optimizer='sgd', pretrain_dataset='jester', pretrain_path='/misc/dl001/bega/Real-time-GesRec/report/jester_resnext_101_RGB_32.pth', resnet_shortcut='B', resnext_cardinality=32, result_path='/misc/dl001/bega/Real-time-GesRec/results', resume_path='', root_path='/misc/dl001/', sample_duration=32, sample_size=112, scale_in_test=1.0, scale_step=0.84089641525, scales=[1.0, 0.84089641525, 0.7071067811803005, 0.5946035574934808, 0.4999999999911653], std=[38.7568578, 37.88248729, 40.02898126], std_norm=False, store_name='model_clfjes_resnext-101', test=False, test_subset='test', train_crop='random', train_validate=False, video_path='/misc/dl001/dataset/NVIDIA/nvgesture_arch', weight_decay=0.001, weighted=False, wide_resnet_k=2)
loading pretrained model /misc/dl001/bega/Real-time-GesRec/report/jester_resnext_101_RGB_32.pth
[INFO]: Converting the pretrained model to Depth init model
[INFO]: Done. Flow model ready.
DataParallel(
  (module): ResNeXt(
    (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)
    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): ResNeXtBottleneck(
        (conv1): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): ResNeXtBottleneck(
        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): ResNeXtBottleneck(
        (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): ResNeXtBottleneck(
        (conv1): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): ResNeXtBottleneck(
        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): ResNeXtBottleneck(
        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): ResNeXtBottleneck(
        (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): ResNeXtBottleneck(
        (conv1): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
          (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (6): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (7): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (8): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (9): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (10): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (11): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (12): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (13): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (14): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (15): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (16): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (17): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (18): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (19): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (20): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (21): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (22): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): ResNeXtBottleneck(
        (conv1): Conv3d(1024, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
          (1): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): ResNeXtBottleneck(
        (conv1): Conv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): ResNeXtBottleneck(
        (conv1): Conv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)
        (bn2): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AvgPool3d(kernel_size=(2, 4, 4), stride=1, padding=0)
    (fc): Linear(in_features=2048, out_features=25, bias=True)
  )
)
Total number of trainable parameters:  47528537
[INFO]: NV Dataset - training is loading...
dataset loading [0/1050]
dataset loading [1000/1050]
[INFO]: NV Dataset - validation is loading...
dataset loading [0/482]
run
train at epoch 1
/misc/dl001/bega/Real-time-GesRec/models/resnext.py:121: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')
Epoch: [1][0/132]	 lr: 0.01000	Time 3.869 (3.869)	Data 3.022 (3.022)	Loss 3.2447 (3.2447)	Acc 0.000 (0.000)	Precision 0.000(0.000)	Recall 0.000(0.000)
Epoch: [1][10/132]	 lr: 0.01000	Time 0.803 (1.081)	Data 0.000 (0.275)	Loss 3.5654 (3.3522)	Acc 0.125 (0.034)	Precision 0.045(0.013)	Recall 0.091(0.028)
Epoch: [1][20/132]	 lr: 0.01000	Time 0.807 (0.950)	Data 0.000 (0.144)	Loss 3.0479 (3.3463)	Acc 0.000 (0.048)	Precision 0.000(0.018)	Recall 0.000(0.036)
Epoch: [1][30/132]	 lr: 0.01000	Time 0.809 (0.905)	Data 0.000 (0.098)	Loss 2.6169 (3.3055)	Acc 0.250 (0.065)	Precision 0.167(0.036)	Recall 0.125(0.047)
Epoch: [1][40/132]	 lr: 0.01000	Time 0.812 (0.882)	Data 0.000 (0.074)	Loss 2.8727 (3.2343)	Acc 0.125 (0.085)	Precision 0.100(0.046)	Recall 0.100(0.059)
Epoch: [1][50/132]	 lr: 0.01000	Time 0.933 (0.885)	Data 0.000 (0.059)	Loss 3.0255 (3.1741)	Acc 0.125 (0.093)	Precision 0.083(0.052)	Recall 0.083(0.064)
Epoch: [1][60/132]	 lr: 0.01000	Time 0.933 (0.893)	Data 0.000 (0.050)	Loss 2.6497 (3.0988)	Acc 0.250 (0.102)	Precision 0.167(0.059)	Recall 0.222(0.071)
Epoch: [1][70/132]	 lr: 0.01000	Time 0.932 (0.898)	Data 0.000 (0.043)	Loss 2.5354 (3.0328)	Acc 0.000 (0.114)	Precision 0.000(0.069)	Recall 0.000(0.077)
Epoch: [1][80/132]	 lr: 0.01000	Time 0.932 (0.903)	Data 0.000 (0.037)	Loss 2.4904 (2.9903)	Acc 0.125 (0.123)	Precision 0.071(0.076)	Recall 0.071(0.082)
Epoch: [1][90/132]	 lr: 0.01000	Time 0.933 (0.906)	Data 0.000 (0.033)	Loss 2.7664 (2.9237)	Acc 0.000 (0.139)	Precision 0.000(0.086)	Recall 0.000(0.092)
Epoch: [1][100/132]	 lr: 0.01000	Time 0.934 (0.908)	Data 0.000 (0.030)	Loss 2.7884 (2.8845)	Acc 0.250 (0.146)	Precision 0.154(0.091)	Recall 0.154(0.097)
Epoch: [1][110/132]	 lr: 0.01000	Time 0.935 (0.910)	Data 0.000 (0.027)	Loss 1.8657 (2.8361)	Acc 0.500 (0.163)	Precision 0.364(0.104)	Recall 0.364(0.109)
Epoch: [1][120/132]	 lr: 0.01000	Time 0.934 (0.912)	Data 0.000 (0.025)	Loss 2.4260 (2.7880)	Acc 0.500 (0.178)	Precision 0.333(0.115)	Recall 0.333(0.119)
Epoch: [1][130/132]	 lr: 0.01000	Time 0.934 (0.914)	Data 0.000 (0.023)	Loss 1.8750 (2.7306)	Acc 0.250 (0.194)	Precision 0.182(0.129)	Recall 0.182(0.133)
validation at epoch 1
/misc/dl10/bega/environs/p35t10c8/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/misc/dl10/bega/environs/p35t10c8/lib/python3.5/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)
Epoch: [1][1/61]	Time 2.75097 (2.75097)	Data 2.53961 (2.53961)	Loss 3.1553 (3.1553)	Acc 0.000 (0.000)
Epoch: [1][2/61]	Time 0.19440 (1.47268)	Data 0.00017 (1.26989)	Loss 1.5996 (2.3774)	Acc 0.250 (0.125)
Epoch: [1][3/61]	Time 0.19083 (1.04540)	Data 0.00012 (0.84663)	Loss 1.3390 (2.0313)	Acc 0.500 (0.250)
Epoch: [1][4/61]	Time 0.19196 (0.83204)	Data 0.00016 (0.63501)	Loss 2.0270 (2.0302)	Acc 0.250 (0.250)
Epoch: [1][5/61]	Time 0.19353 (0.70434)	Data 0.00011 (0.50803)	Loss 2.6296 (2.1501)	Acc 0.125 (0.225)
Epoch: [1][6/61]	Time 0.19509 (0.61946)	Data 0.00009 (0.42338)	Loss 1.3881 (2.0231)	Acc 0.625 (0.292)
Epoch: [1][7/61]	Time 0.19767 (0.55921)	Data 0.00011 (0.36291)	Loss 1.4648 (1.9433)	Acc 0.750 (0.357)
Epoch: [1][8/61]	Time 0.19897 (0.51418)	Data 0.00007 (0.31755)	Loss 1.6110 (1.9018)	Acc 0.125 (0.328)
Epoch: [1][9/61]	Time 0.20014 (0.47929)	Data 0.00011 (0.28228)	Loss 1.7651 (1.8866)	Acc 0.375 (0.333)
Epoch: [1][10/61]	Time 0.20187 (0.45154)	Data 0.00009 (0.25406)	Loss 2.0939 (1.9073)	Acc 0.250 (0.325)
Epoch: [1][11/61]	Time 0.20503 (0.42913)	Data 0.00010 (0.23098)	Loss 2.5386 (1.9647)	Acc 0.375 (0.330)
Epoch: [1][12/61]	Time 0.20671 (0.41060)	Data 0.00013 (0.21174)	Loss 2.0206 (1.9694)	Acc 0.250 (0.323)
Epoch: [1][13/61]	Time 0.20827 (0.39503)	Data 0.00011 (0.19546)	Loss 2.0228 (1.9735)	Acc 0.125 (0.308)
Epoch: [1][14/61]	Time 0.20970 (0.38180)	Data 0.00012 (0.18151)	Loss 2.1544 (1.9864)	Acc 0.250 (0.304)
Epoch: [1][15/61]	Time 0.21227 (0.37049)	Data 0.00009 (0.16941)	Loss 2.3363 (2.0097)	Acc 0.125 (0.292)
Epoch: [1][16/61]	Time 0.21213 (0.36060)	Data 0.00011 (0.15883)	Loss 1.7846 (1.9957)	Acc 0.250 (0.289)
Epoch: [1][17/61]	Time 0.21176 (0.35184)	Data 0.00012 (0.14949)	Loss 1.3046 (1.9550)	Acc 0.500 (0.301)
Epoch: [1][18/61]	Time 0.21480 (0.34423)	Data 0.00019 (0.14120)	Loss 1.5417 (1.9321)	Acc 0.500 (0.312)
Epoch: [1][19/61]	Time 0.21417 (0.33738)	Data 0.00018 (0.13378)	Loss 1.8766 (1.9291)	Acc 0.500 (0.322)
Epoch: [1][20/61]	Time 0.21443 (0.33124)	Data 0.00017 (0.12710)	Loss 2.2810 (1.9467)	Acc 0.375 (0.325)
Epoch: [1][21/61]	Time 0.21629 (0.32576)	Data 0.00007 (0.12105)	Loss 1.9732 (1.9480)	Acc 0.375 (0.327)
Epoch: [1][22/61]	Time 0.21607 (0.32078)	Data 0.00010 (0.11555)	Loss 2.2881 (1.9634)	Acc 0.250 (0.324)
Epoch: [1][23/61]	Time 0.21575 (0.31621)	Data 0.00011 (0.11053)	Loss 2.3275 (1.9793)	Acc 0.375 (0.326)
Epoch: [1][24/61]	Time 0.21573 (0.31202)	Data 0.00009 (0.10593)	Loss 1.5704 (1.9622)	Acc 0.375 (0.328)
Epoch: [1][25/61]	Time 0.21584 (0.30818)	Data 0.00009 (0.10170)	Loss 2.3032 (1.9759)	Acc 0.250 (0.325)
Epoch: [1][26/61]	Time 0.21557 (0.30461)	Data 0.00009 (0.09779)	Loss 2.0171 (1.9775)	Acc 0.375 (0.327)
Epoch: [1][27/61]	Time 0.21554 (0.30132)	Data 0.00009 (0.09417)	Loss 1.2516 (1.9506)	Acc 0.500 (0.333)
Epoch: [1][28/61]	Time 0.21624 (0.29828)	Data 0.00009 (0.09081)	Loss 2.2795 (1.9623)	Acc 0.250 (0.330)
Epoch: [1][29/61]	Time 0.21613 (0.29544)	Data 0.00009 (0.08768)	Loss 2.1409 (1.9685)	Acc 0.250 (0.328)
Epoch: [1][30/61]	Time 0.21521 (0.29277)	Data 0.00006 (0.08476)	Loss 1.5962 (1.9561)	Acc 0.375 (0.329)
Epoch: [1][31/61]	Time 0.21550 (0.29028)	Data 0.00006 (0.08203)	Loss 1.6276 (1.9455)	Acc 0.500 (0.335)
Epoch: [1][32/61]	Time 0.21564 (0.28794)	Data 0.00006 (0.07947)	Loss 1.6999 (1.9378)	Acc 0.375 (0.336)
Epoch: [1][33/61]	Time 0.21523 (0.28574)	Data 0.00008 (0.07706)	Loss 1.8247 (1.9344)	Acc 0.500 (0.341)
Epoch: [1][34/61]	Time 0.21515 (0.28366)	Data 0.00007 (0.07480)	Loss 2.2199 (1.9428)	Acc 0.375 (0.342)
Epoch: [1][35/61]	Time 0.21518 (0.28171)	Data 0.00006 (0.07266)	Loss 1.4424 (1.9285)	Acc 0.625 (0.350)
Epoch: [1][36/61]	Time 0.21496 (0.27985)	Data 0.00006 (0.07064)	Loss 2.0657 (1.9323)	Acc 0.250 (0.347)
Epoch: [1][37/61]	Time 0.21523 (0.27811)	Data 0.00005 (0.06874)	Loss 1.4602 (1.9195)	Acc 0.500 (0.351)
Epoch: [1][38/61]	Time 0.21492 (0.27644)	Data 0.00004 (0.06693)	Loss 2.5264 (1.9355)	Acc 0.375 (0.352)
Epoch: [1][39/61]	Time 0.21495 (0.27487)	Data 0.00004 (0.06521)	Loss 1.4689 (1.9235)	Acc 0.750 (0.362)
Epoch: [1][40/61]	Time 0.21543 (0.27338)	Data 0.00004 (0.06358)	Loss 1.2164 (1.9059)	Acc 0.625 (0.369)
Epoch: [1][41/61]	Time 0.21520 (0.27196)	Data 0.00003 (0.06203)	Loss 2.1249 (1.9112)	Acc 0.250 (0.366)
Epoch: [1][42/61]	Time 0.21548 (0.27062)	Data 0.00003 (0.06056)	Loss 2.0728 (1.9150)	Acc 0.125 (0.360)
Epoch: [1][43/61]	Time 0.21530 (0.26933)	Data 0.00005 (0.05915)	Loss 2.5770 (1.9304)	Acc 0.250 (0.358)
Epoch: [1][44/61]	Time 0.21522 (0.26810)	Data 0.00004 (0.05781)	Loss 2.2143 (1.9369)	Acc 0.125 (0.352)
Epoch: [1][45/61]	Time 0.21485 (0.26692)	Data 0.00007 (0.05652)	Loss 2.4696 (1.9487)	Acc 0.250 (0.350)
Epoch: [1][46/61]	Time 0.21552 (0.26580)	Data 0.00006 (0.05530)	Loss 1.6726 (1.9427)	Acc 0.500 (0.353)
Epoch: [1][47/61]	Time 0.21515 (0.26472)	Data 0.00006 (0.05412)	Loss 1.6713 (1.9370)	Acc 0.375 (0.354)
Epoch: [1][48/61]	Time 0.21521 (0.26369)	Data 0.00005 (0.05299)	Loss 1.3939 (1.9256)	Acc 0.375 (0.354)
Epoch: [1][49/61]	Time 0.21552 (0.26271)	Data 0.00005 (0.05191)	Loss 1.5555 (1.9181)	Acc 0.500 (0.357)
Epoch: [1][50/61]	Time 0.21513 (0.26176)	Data 0.00007 (0.05088)	Loss 1.9902 (1.9195)	Acc 0.375 (0.357)
Epoch: [1][51/61]	Time 0.21505 (0.26084)	Data 0.00005 (0.04988)	Loss 2.2853 (1.9267)	Acc 0.125 (0.353)
Epoch: [1][52/61]	Time 0.21510 (0.25996)	Data 0.00004 (0.04892)	Loss 1.9156 (1.9265)	Acc 0.250 (0.351)
Epoch: [1][53/61]	Time 0.21535 (0.25912)	Data 0.00004 (0.04800)	Loss 1.8676 (1.9254)	Acc 0.625 (0.356)
Epoch: [1][54/61]	Time 0.21529 (0.25831)	Data 0.00005 (0.04711)	Loss 1.9872 (1.9265)	Acc 0.375 (0.356)
Epoch: [1][55/61]	Time 0.21526 (0.25753)	Data 0.00005 (0.04626)	Loss 1.1373 (1.9122)	Acc 0.500 (0.359)
Epoch: [1][56/61]	Time 0.21517 (0.25677)	Data 0.00005 (0.04543)	Loss 2.3814 (1.9205)	Acc 0.250 (0.357)
Epoch: [1][57/61]	Time 0.21512 (0.25604)	Data 0.00005 (0.04464)	Loss 2.1699 (1.9249)	Acc 0.375 (0.357)
Epoch: [1][58/61]	Time 0.21514 (0.25533)	Data 0.00005 (0.04387)	Loss 1.5244 (1.9180)	Acc 0.375 (0.358)
Epoch: [1][59/61]	Time 0.21524 (0.25465)	Data 0.00007 (0.04312)	Loss 2.3897 (1.9260)	Acc 0.375 (0.358)
Epoch: [1][60/61]	Time 0.21533 (0.25400)	Data 0.00005 (0.04241)	Loss 2.8277 (1.9410)	Acc 0.125 (0.354)
Epoch: [1][61/61]	Time 0.07169 (0.25101)	Data 0.00002 (0.04171)	Loss 1.2549 (1.9382)	Acc 0.500 (0.355)
     Valid acc: 0.35477178423236516  (0, ep: 0)
train at epoch 2
Epoch: [2][0/132]	 lr: 0.01000	Time 3.714 (3.714)	Data 2.873 (2.873)	Loss 1.8470 (1.8470)	Acc 0.500 (0.500)	Precision 0.333(0.333)	Recall 0.444(0.444)
Epoch: [2][10/132]	 lr: 0.01000	Time 0.827 (1.077)	Data 0.000 (0.261)	Loss 2.1813 (2.2297)	Acc 0.250 (0.318)	Precision 0.182(0.207)	Recall 0.182(0.213)
Epoch: [2][20/132]	 lr: 0.01000	Time 0.933 (1.003)	Data 0.000 (0.137)	Loss 2.0711 (2.1556)	Acc 0.375 (0.339)	Precision 0.333(0.236)	Recall 0.333(0.235)
Epoch: [2][30/132]	 lr: 0.01000	Time 0.932 (0.980)	Data 0.000 (0.093)	Loss 1.8714 (2.1762)	Acc 0.625 (0.355)	Precision 0.625(0.247)	Recall 0.500(0.244)
Epoch: [2][40/132]	 lr: 0.01000	Time 0.932 (0.968)	Data 0.000 (0.070)	Loss 2.1884 (2.1546)	Acc 0.250 (0.357)	Precision 0.167(0.248)	Recall 0.167(0.244)
Epoch: [2][50/132]	 lr: 0.01000	Time 0.933 (0.961)	Data 0.000 (0.056)	Loss 2.0669 (2.1367)	Acc 0.250 (0.350)	Precision 0.182(0.245)	Recall 0.182(0.238)
Epoch: [2][60/132]	 lr: 0.01000	Time 0.934 (0.956)	Data 0.000 (0.047)	Loss 1.3369 (2.1320)	Acc 0.500 (0.357)	Precision 0.333(0.248)	Recall 0.333(0.240)
Epoch: [2][70/132]	 lr: 0.01000	Time 0.934 (0.953)	Data 0.000 (0.041)	Loss 2.4777 (2.1230)	Acc 0.250 (0.359)	Precision 0.182(0.249)	Recall 0.182(0.243)
Epoch: [2][80/132]	 lr: 0.01000	Time 0.933 (0.951)	Data 0.000 (0.036)	Loss 1.5976 (2.0803)	Acc 0.500 (0.366)	Precision 0.350(0.254)	Recall 0.400(0.249)
Epoch: [2][90/132]	 lr: 0.01000	Time 0.932 (0.949)	Data 0.000 (0.032)	Loss 2.8687 (2.0696)	Acc 0.250 (0.374)	Precision 0.167(0.260)	Recall 0.125(0.256)
Epoch: [2][100/132]	 lr: 0.01000	Time 0.934 (0.947)	Data 0.000 (0.029)	Loss 1.7372 (2.0761)	Acc 0.500 (0.376)	Precision 0.400(0.265)	Recall 0.400(0.259)
Epoch: [2][110/132]	 lr: 0.01000	Time 0.933 (0.946)	Data 0.000 (0.026)	Loss 1.8561 (2.0600)	Acc 0.500 (0.384)	Precision 0.333(0.269)	Recall 0.296(0.263)
Epoch: [2][120/132]	 lr: 0.01000	Time 0.932 (0.945)	Data 0.000 (0.024)	Loss 1.6070 (2.0544)	Acc 0.250 (0.381)	Precision 0.143(0.268)	Recall 0.143(0.262)
Epoch: [2][130/132]	 lr: 0.01000	Time 0.933 (0.944)	Data 0.000 (0.022)	Loss 1.9818 (2.0308)	Acc 0.500 (0.385)	Precision 0.400(0.272)	Recall 0.400(0.268)
validation at epoch 2
Epoch: [2][1/61]	Time 2.74209 (2.74209)	Data 2.53138 (2.53138)	Loss 2.8516 (2.8516)	Acc 0.375 (0.375)
Epoch: [2][2/61]	Time 0.19435 (1.46822)	Data 0.00010 (1.26574)	Loss 1.1793 (2.0155)	Acc 0.625 (0.500)
Epoch: [2][3/61]	Time 0.19070 (1.04238)	Data 0.00011 (0.84386)	Loss 1.7926 (1.9412)	Acc 0.375 (0.458)
Epoch: [2][4/61]	Time 0.19149 (0.82966)	Data 0.00012 (0.63293)	Loss 1.2264 (1.7625)	Acc 0.500 (0.469)
Epoch: [2][5/61]	Time 0.19127 (0.70198)	Data 0.00009 (0.50636)	Loss 1.4797 (1.7059)	Acc 0.625 (0.500)
Epoch: [2][6/61]	Time 0.19361 (0.61725)	Data 0.00009 (0.42198)	Loss 1.0994 (1.6048)	Acc 0.750 (0.542)
Epoch: [2][7/61]	Time 0.19589 (0.55706)	Data 0.00015 (0.36172)	Loss 1.0496 (1.5255)	Acc 0.750 (0.571)
Epoch: [2][8/61]	Time 0.19580 (0.51190)	Data 0.00007 (0.31651)	Loss 1.1813 (1.4825)	Acc 0.500 (0.562)
Epoch: [2][9/61]	Time 0.19920 (0.47715)	Data 0.00009 (0.28136)	Loss 1.1004 (1.4400)	Acc 0.625 (0.569)
Epoch: [2][10/61]	Time 0.20042 (0.44948)	Data 0.00011 (0.25323)	Loss 1.4347 (1.4395)	Acc 0.750 (0.588)
Epoch: [2][11/61]	Time 0.20146 (0.42693)	Data 0.00009 (0.23022)	Loss 2.6283 (1.5476)	Acc 0.375 (0.568)
Epoch: [2][12/61]	Time 0.20480 (0.40842)	Data 0.00009 (0.21104)	Loss 1.3226 (1.5288)	Acc 0.500 (0.562)
Epoch: [2][13/61]	Time 0.20580 (0.39284)	Data 0.00008 (0.19481)	Loss 1.1635 (1.5007)	Acc 0.625 (0.567)
Epoch: [2][14/61]	Time 0.20791 (0.37963)	Data 0.00009 (0.18090)	Loss 1.5906 (1.5071)	Acc 0.500 (0.562)
Epoch: [2][15/61]	Time 0.20971 (0.36830)	Data 0.00009 (0.16885)	Loss 2.1340 (1.5489)	Acc 0.375 (0.550)
Epoch: [2][16/61]	Time 0.21164 (0.35851)	Data 0.00012 (0.15831)	Loss 1.4627 (1.5435)	Acc 0.750 (0.562)
Epoch: [2][17/61]	Time 0.21252 (0.34992)	Data 0.00016 (0.14900)	Loss 0.8667 (1.5037)	Acc 0.750 (0.574)
Epoch: [2][18/61]	Time 0.21375 (0.34236)	Data 0.00011 (0.14073)	Loss 1.0868 (1.4806)	Acc 0.625 (0.576)
Epoch: [2][19/61]	Time 0.21594 (0.33570)	Data 0.00012 (0.13333)	Loss 1.2423 (1.4680)	Acc 0.625 (0.579)
Epoch: [2][20/61]	Time 0.21654 (0.32974)	Data 0.00012 (0.12667)	Loss 1.3249 (1.4609)	Acc 0.625 (0.581)
Epoch: [2][21/61]	Time 0.21600 (0.32433)	Data 0.00007 (0.12064)	Loss 1.0103 (1.4394)	Acc 0.750 (0.589)
Epoch: [2][22/61]	Time 0.21650 (0.31943)	Data 0.00009 (0.11516)	Loss 1.9089 (1.4607)	Acc 0.375 (0.580)
Epoch: [2][23/61]	Time 0.21637 (0.31495)	Data 0.00010 (0.11016)	Loss 1.6900 (1.4707)	Acc 0.375 (0.571)
Epoch: [2][24/61]	Time 0.21580 (0.31081)	Data 0.00009 (0.10557)	Loss 1.7318 (1.4816)	Acc 0.375 (0.562)
Epoch: [2][25/61]	Time 0.21646 (0.30704)	Data 0.00010 (0.10135)	Loss 1.0496 (1.4643)	Acc 0.750 (0.570)
Epoch: [2][26/61]	Time 0.21640 (0.30355)	Data 0.00011 (0.09746)	Loss 1.3695 (1.4607)	Acc 0.625 (0.572)
Epoch: [2][27/61]	Time 0.21687 (0.30034)	Data 0.00009 (0.09385)	Loss 1.0352 (1.4449)	Acc 0.625 (0.574)
Epoch: [2][28/61]	Time 0.21589 (0.29733)	Data 0.00009 (0.09050)	Loss 1.5505 (1.4487)	Acc 0.625 (0.576)
Epoch: [2][29/61]	Time 0.21580 (0.29452)	Data 0.00008 (0.08739)	Loss 1.3574 (1.4455)	Acc 0.375 (0.569)
Epoch: [2][30/61]	Time 0.21584 (0.29189)	Data 0.00006 (0.08448)	Loss 1.2774 (1.4399)	Acc 0.625 (0.571)
Epoch: [2][31/61]	Time 0.21601 (0.28945)	Data 0.00005 (0.08175)	Loss 1.5012 (1.4419)	Acc 0.375 (0.565)
Epoch: [2][32/61]	Time 0.21616 (0.28716)	Data 0.00006 (0.07920)	Loss 1.2063 (1.4345)	Acc 0.250 (0.555)
Epoch: [2][33/61]	Time 0.21584 (0.28499)	Data 0.00011 (0.07680)	Loss 1.1592 (1.4262)	Acc 0.875 (0.564)
Epoch: [2][34/61]	Time 0.21539 (0.28295)	Data 0.00004 (0.07454)	Loss 1.1190 (1.4172)	Acc 0.625 (0.566)
Epoch: [2][35/61]	Time 0.21566 (0.28102)	Data 0.00003 (0.07242)	Loss 1.0821 (1.4076)	Acc 0.750 (0.571)
Epoch: [2][36/61]	Time 0.21536 (0.27920)	Data 0.00005 (0.07041)	Loss 1.4712 (1.4094)	Acc 0.625 (0.573)
Epoch: [2][37/61]	Time 0.21558 (0.27748)	Data 0.00004 (0.06850)	Loss 1.2259 (1.4044)	Acc 0.500 (0.571)
Epoch: [2][38/61]	Time 0.21582 (0.27586)	Data 0.00005 (0.06670)	Loss 1.3701 (1.4035)	Acc 0.500 (0.569)
Epoch: [2][39/61]	Time 0.21621 (0.27433)	Data 0.00004 (0.06499)	Loss 1.4820 (1.4055)	Acc 0.250 (0.561)
Epoch: [2][40/61]	Time 0.21562 (0.27286)	Data 0.00004 (0.06337)	Loss 1.5713 (1.4097)	Acc 0.625 (0.562)
Epoch: [2][41/61]	Time 0.21600 (0.27147)	Data 0.00005 (0.06182)	Loss 1.3408 (1.4080)	Acc 0.500 (0.561)
Epoch: [2][42/61]	Time 0.21562 (0.27014)	Data 0.00004 (0.06035)	Loss 2.1162 (1.4248)	Acc 0.125 (0.551)
Epoch: [2][43/61]	Time 0.21521 (0.26887)	Data 0.00006 (0.05895)	Loss 1.5285 (1.4272)	Acc 0.375 (0.547)
Epoch: [2][44/61]	Time 0.21554 (0.26766)	Data 0.00004 (0.05761)	Loss 1.3419 (1.4253)	Acc 0.500 (0.545)
Epoch: [2][45/61]	Time 0.21566 (0.26650)	Data 0.00007 (0.05633)	Loss 1.9265 (1.4364)	Acc 0.375 (0.542)
Epoch: [2][46/61]	Time 0.21570 (0.26540)	Data 0.00007 (0.05511)	Loss 0.6447 (1.4192)	Acc 0.875 (0.549)
Epoch: [2][47/61]	Time 0.21572 (0.26434)	Data 0.00005 (0.05394)	Loss 1.3884 (1.4186)	Acc 0.625 (0.551)
Epoch: [2][48/61]	Time 0.21554 (0.26332)	Data 0.00005 (0.05282)	Loss 1.3421 (1.4170)	Acc 0.750 (0.555)
Epoch: [2][49/61]	Time 0.21585 (0.26235)	Data 0.00006 (0.05174)	Loss 1.6462 (1.4217)	Acc 0.375 (0.551)
Epoch: [2][50/61]	Time 0.21576 (0.26142)	Data 0.00004 (0.05071)	Loss 1.0972 (1.4152)	Acc 0.750 (0.555)
Epoch: [2][51/61]	Time 0.21563 (0.26052)	Data 0.00005 (0.04971)	Loss 2.0719 (1.4281)	Acc 0.375 (0.551)
Epoch: [2][52/61]	Time 0.21591 (0.25967)	Data 0.00006 (0.04876)	Loss 1.4479 (1.4284)	Acc 0.375 (0.548)
Epoch: [2][53/61]	Time 0.21570 (0.25884)	Data 0.00004 (0.04784)	Loss 1.4631 (1.4291)	Acc 0.500 (0.547)
Epoch: [2][54/61]	Time 0.21540 (0.25803)	Data 0.00003 (0.04695)	Loss 0.6727 (1.4151)	Acc 0.750 (0.551)
Epoch: [2][55/61]	Time 0.21533 (0.25725)	Data 0.00006 (0.04610)	Loss 0.7640 (1.4032)	Acc 0.875 (0.557)
Epoch: [2][56/61]	Time 0.21592 (0.25652)	Data 0.00005 (0.04528)	Loss 2.0200 (1.4143)	Acc 0.250 (0.551)
Epoch: [2][57/61]	Time 0.21541 (0.25580)	Data 0.00005 (0.04448)	Loss 1.8030 (1.4211)	Acc 0.625 (0.553)
Epoch: [2][58/61]	Time 0.21612 (0.25511)	Data 0.00005 (0.04372)	Loss 1.5405 (1.4231)	Acc 0.500 (0.552)
Epoch: [2][59/61]	Time 0.21594 (0.25445)	Data 0.00006 (0.04298)	Loss 2.1545 (1.4355)	Acc 0.500 (0.551)
Epoch: [2][60/61]	Time 0.21607 (0.25381)	Data 0.00005 (0.04226)	Loss 2.0282 (1.4454)	Acc 0.375 (0.548)
Epoch: [2][61/61]	Time 0.07153 (0.25082)	Data 0.00003 (0.04157)	Loss 0.8954 (1.4431)	Acc 1.000 (0.550)
     Valid acc: 0.549792531120332  (0.35477178423236516, ep: 1)
train at epoch 3
Epoch: [3][0/132]	 lr: 0.01000	Time 3.649 (3.649)	Data 2.813 (2.813)	Loss 1.8754 (1.8754)	Acc 0.250 (0.250)	Precision 0.167(0.167)	Recall 0.125(0.125)
Epoch: [3][10/132]	 lr: 0.01000	Time 0.813 (1.070)	Data 0.000 (0.256)	Loss 1.7059 (1.7556)	Acc 0.375 (0.466)	Precision 0.227(0.341)	Recall 0.273(0.347)
Epoch: [3][20/132]	 lr: 0.01000	Time 0.938 (0.991)	Data 0.000 (0.134)	Loss 2.8368 (1.8188)	Acc 0.000 (0.435)	Precision 0.000(0.316)	Recall 0.000(0.310)
Epoch: [3][30/132]	 lr: 0.01000	Time 0.931 (0.972)	Data 0.000 (0.091)	Loss 1.7514 (1.8243)	Acc 0.625 (0.423)	Precision 0.444(0.305)	Recall 0.389(0.296)
Epoch: [3][40/132]	 lr: 0.01000	Time 0.933 (0.963)	Data 0.000 (0.069)	Loss 2.4111 (1.8081)	Acc 0.500 (0.430)	Precision 0.500(0.317)	Recall 0.417(0.304)
Epoch: [3][50/132]	 lr: 0.01000	Time 0.932 (0.957)	Data 0.000 (0.055)	Loss 2.4204 (1.7887)	Acc 0.125 (0.439)	Precision 0.091(0.324)	Recall 0.091(0.314)
Epoch: [3][60/132]	 lr: 0.01000	Time 0.933 (0.953)	Data 0.000 (0.046)	Loss 1.8969 (1.7897)	Acc 0.500 (0.445)	Precision 0.333(0.329)	Recall 0.333(0.315)
Epoch: [3][70/132]	 lr: 0.01000	Time 0.932 (0.950)	Data 0.000 (0.040)	Loss 1.4464 (1.7720)	Acc 0.500 (0.445)	Precision 0.333(0.329)	Recall 0.444(0.322)
Epoch: [3][80/132]	 lr: 0.01000	Time 0.934 (0.948)	Data 0.000 (0.035)	Loss 2.0364 (1.8054)	Acc 0.375 (0.435)	Precision 0.273(0.320)	Recall 0.227(0.311)
Epoch: [3][90/132]	 lr: 0.01000	Time 0.932 (0.946)	Data 0.000 (0.031)	Loss 1.7364 (1.8077)	Acc 0.500 (0.431)	Precision 0.364(0.318)	Recall 0.364(0.310)
Epoch: [3][100/132]	 lr: 0.01000	Time 0.933 (0.945)	Data 0.000 (0.028)	Loss 1.4239 (1.7980)	Acc 0.375 (0.434)	Precision 0.250(0.322)	Recall 0.208(0.312)
Epoch: [3][110/132]	 lr: 0.01000	Time 0.933 (0.944)	Data 0.000 (0.025)	Loss 1.3396 (1.7833)	Acc 0.625 (0.440)	Precision 0.400(0.324)	Recall 0.400(0.313)
Epoch: [3][120/132]	 lr: 0.01000	Time 0.934 (0.943)	Data 0.000 (0.023)	Loss 1.4510 (1.7920)	Acc 0.500 (0.431)	Precision 0.500(0.317)	Recall 0.375(0.305)
Epoch: [3][130/132]	 lr: 0.01000	Time 0.932 (0.942)	Data 0.000 (0.022)	Loss 0.9927 (1.7855)	Acc 0.625 (0.434)	Precision 0.444(0.320)	Recall 0.556(0.309)
validation at epoch 3
Epoch: [3][1/61]	Time 2.84591 (2.84591)	Data 2.63431 (2.63431)	Loss 2.8980 (2.8980)	Acc 0.375 (0.375)
Epoch: [3][2/61]	Time 0.19419 (1.52005)	Data 0.00013 (1.31722)	Loss 1.0060 (1.9520)	Acc 0.625 (0.500)
Epoch: [3][3/61]	Time 0.18994 (1.07668)	Data 0.00012 (0.87819)	Loss 1.4664 (1.7901)	Acc 0.625 (0.542)
Epoch: [3][4/61]	Time 0.18969 (0.85493)	Data 0.00011 (0.65867)	Loss 1.1836 (1.6385)	Acc 0.500 (0.531)
Epoch: [3][5/61]	Time 0.19049 (0.72204)	Data 0.00009 (0.52695)	Loss 1.4530 (1.6014)	Acc 0.625 (0.550)
Epoch: [3][6/61]	Time 0.19203 (0.63371)	Data 0.00009 (0.43914)	Loss 1.0896 (1.5161)	Acc 0.500 (0.542)
Epoch: [3][7/61]	Time 0.19514 (0.57106)	Data 0.00010 (0.37642)	Loss 1.2419 (1.4769)	Acc 0.750 (0.571)
Epoch: [3][8/61]	Time 0.19516 (0.52407)	Data 0.00012 (0.32938)	Loss 1.1931 (1.4415)	Acc 0.500 (0.562)
Epoch: [3][9/61]	Time 0.19718 (0.48775)	Data 0.00010 (0.29280)	Loss 1.2975 (1.4255)	Acc 0.625 (0.569)
Epoch: [3][10/61]	Time 0.19896 (0.45887)	Data 0.00009 (0.26353)	Loss 2.8175 (1.5647)	Acc 0.500 (0.562)
Epoch: [3][11/61]	Time 0.20066 (0.43539)	Data 0.00011 (0.23958)	Loss 1.7705 (1.5834)	Acc 0.500 (0.557)
Epoch: [3][12/61]	Time 0.20252 (0.41599)	Data 0.00012 (0.21962)	Loss 0.9054 (1.5269)	Acc 0.625 (0.562)
Epoch: [3][13/61]	Time 0.20512 (0.39977)	Data 0.00010 (0.20274)	Loss 1.2031 (1.5020)	Acc 0.625 (0.567)
Epoch: [3][14/61]	Time 0.20680 (0.38598)	Data 0.00012 (0.18826)	Loss 0.9363 (1.4616)	Acc 0.625 (0.571)
Epoch: [3][15/61]	Time 0.20909 (0.37419)	Data 0.00009 (0.17572)	Loss 1.7442 (1.4804)	Acc 0.375 (0.558)
Epoch: [3][16/61]	Time 0.21042 (0.36396)	Data 0.00011 (0.16474)	Loss 2.0069 (1.5133)	Acc 0.250 (0.539)
Epoch: [3][17/61]	Time 0.21167 (0.35500)	Data 0.00015 (0.15506)	Loss 0.9192 (1.4784)	Acc 0.750 (0.551)
Epoch: [3][18/61]	Time 0.21327 (0.34712)	Data 0.00014 (0.14645)	Loss 2.0194 (1.5084)	Acc 0.500 (0.549)
Epoch: [3][19/61]	Time 0.21382 (0.34011)	Data 0.00013 (0.13875)	Loss 1.0231 (1.4829)	Acc 0.750 (0.559)
Epoch: [3][20/61]	Time 0.21385 (0.33380)	Data 0.00009 (0.13182)	Loss 1.1362 (1.4656)	Acc 0.625 (0.562)
Epoch: [3][21/61]	Time 0.21541 (0.32816)	Data 0.00011 (0.12555)	Loss 0.5193 (1.4205)	Acc 0.875 (0.577)
Epoch: [3][22/61]	Time 0.21711 (0.32311)	Data 0.00012 (0.11985)	Loss 1.5635 (1.4270)	Acc 0.500 (0.574)
Epoch: [3][23/61]	Time 0.21609 (0.31846)	Data 0.00012 (0.11464)	Loss 2.8393 (1.4884)	Acc 0.500 (0.571)
Epoch: [3][24/61]	Time 0.21635 (0.31420)	Data 0.00009 (0.10987)	Loss 2.4800 (1.5297)	Acc 0.250 (0.557)
Epoch: [3][25/61]	Time 0.21606 (0.31028)	Data 0.00010 (0.10548)	Loss 1.1794 (1.5157)	Acc 0.625 (0.560)
Epoch: [3][26/61]	Time 0.21592 (0.30665)	Data 0.00011 (0.10143)	Loss 0.8769 (1.4911)	Acc 0.625 (0.562)
Epoch: [3][27/61]	Time 0.21559 (0.30328)	Data 0.00011 (0.09767)	Loss 0.9466 (1.4710)	Acc 0.625 (0.565)
Epoch: [3][28/61]	Time 0.21535 (0.30014)	Data 0.00009 (0.09419)	Loss 1.3499 (1.4666)	Acc 0.750 (0.571)
Epoch: [3][29/61]	Time 0.21567 (0.29722)	Data 0.00008 (0.09094)	Loss 1.0620 (1.4527)	Acc 0.625 (0.573)
Epoch: [3][30/61]	Time 0.21569 (0.29451)	Data 0.00006 (0.08791)	Loss 0.7929 (1.4307)	Acc 0.750 (0.579)
Epoch: [3][31/61]	Time 0.21577 (0.29197)	Data 0.00005 (0.08508)	Loss 1.0170 (1.4174)	Acc 0.625 (0.581)
Epoch: [3][32/61]	Time 0.21543 (0.28957)	Data 0.00005 (0.08242)	Loss 0.9696 (1.4034)	Acc 0.500 (0.578)
Epoch: [3][33/61]	Time 0.21586 (0.28734)	Data 0.00007 (0.07993)	Loss 1.1913 (1.3969)	Acc 0.875 (0.587)
Epoch: [3][34/61]	Time 0.21591 (0.28524)	Data 0.00005 (0.07758)	Loss 1.1728 (1.3903)	Acc 0.625 (0.588)
Epoch: [3][35/61]	Time 0.21535 (0.28324)	Data 0.00007 (0.07536)	Loss 0.9901 (1.3789)	Acc 0.750 (0.593)
Epoch: [3][36/61]	Time 0.21492 (0.28134)	Data 0.00003 (0.07327)	Loss 1.7571 (1.3894)	Acc 0.500 (0.590)
Epoch: [3][37/61]	Time 0.21507 (0.27955)	Data 0.00004 (0.07129)	Loss 2.0402 (1.4070)	Acc 0.250 (0.581)
Epoch: [3][38/61]	Time 0.21556 (0.27787)	Data 0.00005 (0.06942)	Loss 1.3121 (1.4045)	Acc 0.500 (0.579)
Epoch: [3][39/61]	Time 0.21550 (0.27627)	Data 0.00006 (0.06764)	Loss 1.3285 (1.4026)	Acc 0.500 (0.577)
Epoch: [3][40/61]	Time 0.21497 (0.27474)	Data 0.00003 (0.06595)	Loss 1.9492 (1.4162)	Acc 0.500 (0.575)
Epoch: [3][41/61]	Time 0.21528 (0.27329)	Data 0.00003 (0.06434)	Loss 0.3700 (1.3907)	Acc 0.875 (0.582)
Epoch: [3][42/61]	Time 0.21524 (0.27191)	Data 0.00004 (0.06281)	Loss 1.0255 (1.3820)	Acc 0.500 (0.580)
Epoch: [3][43/61]	Time 0.21484 (0.27058)	Data 0.00003 (0.06135)	Loss 1.8240 (1.3923)	Acc 0.125 (0.570)
Epoch: [3][44/61]	Time 0.21561 (0.26933)	Data 0.00004 (0.05996)	Loss 1.9760 (1.4056)	Acc 0.625 (0.571)
Epoch: [3][45/61]	Time 0.21517 (0.26813)	Data 0.00006 (0.05862)	Loss 2.6916 (1.4341)	Acc 0.250 (0.564)
Epoch: [3][46/61]	Time 0.21570 (0.26699)	Data 0.00005 (0.05735)	Loss 0.4864 (1.4135)	Acc 1.000 (0.573)
Epoch: [3][47/61]	Time 0.21544 (0.26589)	Data 0.00005 (0.05613)	Loss 1.5681 (1.4168)	Acc 0.500 (0.572)
Epoch: [3][48/61]	Time 0.21531 (0.26484)	Data 0.00006 (0.05496)	Loss 1.3786 (1.4160)	Acc 0.375 (0.568)
Epoch: [3][49/61]	Time 0.21561 (0.26383)	Data 0.00005 (0.05384)	Loss 1.1488 (1.4106)	Acc 0.625 (0.569)
Epoch: [3][50/61]	Time 0.21535 (0.26286)	Data 0.00005 (0.05277)	Loss 1.0902 (1.4042)	Acc 0.500 (0.568)
Epoch: [3][51/61]	Time 0.21512 (0.26193)	Data 0.00006 (0.05173)	Loss 2.2164 (1.4201)	Acc 0.375 (0.564)
Epoch: [3][52/61]	Time 0.21503 (0.26102)	Data 0.00003 (0.05074)	Loss 1.3232 (1.4182)	Acc 0.625 (0.565)
Epoch: [3][53/61]	Time 0.21503 (0.26016)	Data 0.00005 (0.04978)	Loss 1.7444 (1.4244)	Acc 0.500 (0.564)
Epoch: [3][54/61]	Time 0.21530 (0.25933)	Data 0.00004 (0.04886)	Loss 0.5958 (1.4090)	Acc 0.750 (0.567)
Epoch: [3][55/61]	Time 0.21537 (0.25853)	Data 0.00007 (0.04797)	Loss 1.3253 (1.4075)	Acc 0.500 (0.566)
Epoch: [3][56/61]	Time 0.21540 (0.25776)	Data 0.00007 (0.04712)	Loss 1.5673 (1.4104)	Acc 0.500 (0.565)
Epoch: [3][57/61]	Time 0.21514 (0.25701)	Data 0.00006 (0.04629)	Loss 1.4327 (1.4108)	Acc 0.500 (0.564)
Epoch: [3][58/61]	Time 0.21470 (0.25628)	Data 0.00006 (0.04550)	Loss 1.2494 (1.4080)	Acc 0.750 (0.567)
Epoch: [3][59/61]	Time 0.21523 (0.25558)	Data 0.00005 (0.04473)	Loss 1.6143 (1.4115)	Acc 0.625 (0.568)
Epoch: [3][60/61]	Time 0.21540 (0.25491)	Data 0.00005 (0.04398)	Loss 2.2143 (1.4249)	Acc 0.375 (0.565)
Epoch: [3][61/61]	Time 0.07163 (0.25191)	Data 0.00002 (0.04326)	Loss 1.3354 (1.4245)	Acc 0.500 (0.564)
     Valid acc: 0.5643153526970954  (0.549792531120332, ep: 2)
train at epoch 4
Epoch: [4][0/132]	 lr: 0.01000	Time 3.684 (3.684)	Data 2.844 (2.844)	Loss 1.2826 (1.2826)	Acc 0.625 (0.625)	Precision 0.500(0.500)	Recall 0.500(0.500)
Epoch: [4][10/132]	 lr: 0.01000	Time 0.811 (1.072)	Data 0.000 (0.259)	Loss 1.6941 (1.6641)	Acc 0.375 (0.466)	Precision 0.250(0.352)	Recall 0.208(0.328)
Epoch: [4][20/132]	 lr: 0.01000	Time 0.931 (0.992)	Data 0.000 (0.136)	Loss 1.8812 (1.7267)	Acc 0.500 (0.452)	Precision 0.350(0.329)	Recall 0.400(0.323)
Epoch: [4][30/132]	 lr: 0.01000	Time 0.932 (0.973)	Data 0.000 (0.092)	Loss 1.4481 (1.7350)	Acc 0.500 (0.427)	Precision 0.300(0.304)	Recall 0.300(0.301)
Epoch: [4][40/132]	 lr: 0.01000	Time 0.932 (0.963)	Data 0.000 (0.070)	Loss 1.6754 (1.7205)	Acc 0.375 (0.442)	Precision 0.300(0.320)	Recall 0.300(0.316)
Epoch: [4][50/132]	 lr: 0.01000	Time 0.933 (0.957)	Data 0.000 (0.056)	Loss 1.6967 (1.6674)	Acc 0.500 (0.449)	Precision 0.364(0.321)	Recall 0.318(0.316)
Epoch: [4][60/132]	 lr: 0.01000	Time 0.934 (0.953)	Data 0.000 (0.047)	Loss 2.2169 (1.6379)	Acc 0.125 (0.467)	Precision 0.083(0.346)	Recall 0.083(0.340)
Epoch: [4][70/132]	 lr: 0.01000	Time 0.931 (0.950)	Data 0.002 (0.040)	Loss 3.0659 (1.6740)	Acc 0.125 (0.458)	Precision 0.042(0.340)	Recall 0.042(0.333)
Epoch: [4][80/132]	 lr: 0.01000	Time 0.932 (0.948)	Data 0.000 (0.035)	Loss 1.0215 (1.6787)	Acc 0.750 (0.461)	Precision 0.625(0.344)	Recall 0.625(0.337)
Epoch: [4][90/132]	 lr: 0.01000	Time 0.932 (0.947)	Data 0.000 (0.031)	Loss 1.2384 (1.6640)	Acc 0.500 (0.467)	Precision 0.318(0.347)	Recall 0.364(0.341)
Epoch: [4][100/132]	 lr: 0.01000	Time 0.934 (0.946)	Data 0.000 (0.028)	Loss 1.6629 (1.6394)	Acc 0.625 (0.474)	Precision 0.375(0.352)	Recall 0.375(0.344)
Epoch: [4][110/132]	 lr: 0.01000	Time 0.934 (0.945)	Data 0.000 (0.026)	Loss 2.1813 (1.6624)	Acc 0.375 (0.471)	Precision 0.273(0.350)	Recall 0.273(0.339)
Epoch: [4][120/132]	 lr: 0.01000	Time 0.935 (0.944)	Data 0.000 (0.024)	Loss 1.5823 (1.6791)	Acc 0.500 (0.472)	Precision 0.364(0.350)	Recall 0.364(0.339)
Epoch: [4][130/132]	 lr: 0.01000	Time 0.935 (0.943)	Data 0.000 (0.022)	Loss 1.9574 (1.6893)	Acc 0.375 (0.467)	Precision 0.231(0.346)	Recall 0.231(0.334)
validation at epoch 4
Epoch: [4][1/61]	Time 2.97136 (2.97136)	Data 2.75759 (2.75759)	Loss 1.9744 (1.9744)	Acc 0.500 (0.500)
Epoch: [4][2/61]	Time 0.19565 (1.58350)	Data 0.00012 (1.37885)	Loss 1.5301 (1.7522)	Acc 0.375 (0.438)
Epoch: [4][3/61]	Time 0.19008 (1.11903)	Data 0.00008 (0.91926)	Loss 1.3171 (1.6072)	Acc 0.375 (0.417)
Epoch: [4][4/61]	Time 0.19019 (0.88682)	Data 0.00009 (0.68947)	Loss 1.7060 (1.6319)	Acc 0.375 (0.406)
Epoch: [4][5/61]	Time 0.19020 (0.74750)	Data 0.00009 (0.55159)	Loss 1.6454 (1.6346)	Acc 0.375 (0.400)
Epoch: [4][6/61]	Time 0.19121 (0.65478)	Data 0.00009 (0.45968)	Loss 0.9696 (1.5238)	Acc 0.500 (0.417)
Epoch: [4][7/61]	Time 0.19254 (0.58875)	Data 0.00011 (0.39402)	Loss 0.8907 (1.4333)	Acc 0.875 (0.482)
Epoch: [4][8/61]	Time 0.19521 (0.53955)	Data 0.00009 (0.34478)	Loss 0.9344 (1.3710)	Acc 0.625 (0.500)
Epoch: [4][9/61]	Time 0.19617 (0.50140)	Data 0.00011 (0.30649)	Loss 1.1021 (1.3411)	Acc 0.625 (0.514)
Epoch: [4][10/61]	Time 0.19826 (0.47109)	Data 0.00009 (0.27585)	Loss 1.7850 (1.3855)	Acc 0.375 (0.500)
Epoch: [4][11/61]	Time 0.19976 (0.44642)	Data 0.00012 (0.25078)	Loss 2.1681 (1.4566)	Acc 0.375 (0.489)
Epoch: [4][12/61]	Time 0.20099 (0.42597)	Data 0.00008 (0.22989)	Loss 0.9034 (1.4105)	Acc 0.500 (0.490)
Epoch: [4][13/61]	Time 0.20314 (0.40883)	Data 0.00009 (0.21221)	Loss 0.8095 (1.3643)	Acc 0.750 (0.510)
Epoch: [4][14/61]	Time 0.20525 (0.39429)	Data 0.00009 (0.19706)	Loss 1.6573 (1.3852)	Acc 0.500 (0.509)
Epoch: [4][15/61]	Time 0.20712 (0.38181)	Data 0.00010 (0.18393)	Loss 1.7882 (1.4121)	Acc 0.500 (0.508)
Epoch: [4][16/61]	Time 0.20871 (0.37099)	Data 0.00009 (0.17244)	Loss 1.6775 (1.4287)	Acc 0.500 (0.508)
Epoch: [4][17/61]	Time 0.21087 (0.36157)	Data 0.00017 (0.16231)	Loss 1.0536 (1.4066)	Acc 0.750 (0.522)
Epoch: [4][18/61]	Time 0.21249 (0.35329)	Data 0.00007 (0.15329)	Loss 0.7179 (1.3683)	Acc 0.625 (0.528)
Epoch: [4][19/61]	Time 0.21594 (0.34606)	Data 0.00009 (0.14523)	Loss 1.1036 (1.3544)	Acc 0.875 (0.546)
Epoch: [4][20/61]	Time 0.21587 (0.33955)	Data 0.00011 (0.13797)	Loss 0.5889 (1.3161)	Acc 0.750 (0.556)
Epoch: [4][21/61]	Time 0.21594 (0.33366)	Data 0.00009 (0.13141)	Loss 0.6836 (1.2860)	Acc 0.875 (0.571)
Epoch: [4][22/61]	Time 0.21636 (0.32833)	Data 0.00009 (0.12544)	Loss 1.5190 (1.2966)	Acc 0.625 (0.574)
Epoch: [4][23/61]	Time 0.21680 (0.32348)	Data 0.00009 (0.11999)	Loss 1.4651 (1.3039)	Acc 0.500 (0.571)
Epoch: [4][24/61]	Time 0.21721 (0.31905)	Data 0.00011 (0.11499)	Loss 1.2859 (1.3032)	Acc 0.500 (0.568)
Epoch: [4][25/61]	Time 0.21635 (0.31495)	Data 0.00013 (0.11040)	Loss 0.9497 (1.2890)	Acc 0.500 (0.565)
Epoch: [4][26/61]	Time 0.21631 (0.31115)	Data 0.00009 (0.10616)	Loss 0.7518 (1.2684)	Acc 0.750 (0.572)
Epoch: [4][27/61]	Time 0.21672 (0.30765)	Data 0.00009 (0.10223)	Loss 0.7175 (1.2480)	Acc 0.750 (0.579)
Epoch: [4][28/61]	Time 0.21676 (0.30441)	Data 0.00011 (0.09858)	Loss 2.2995 (1.2855)	Acc 0.625 (0.580)
Epoch: [4][29/61]	Time 0.21622 (0.30137)	Data 0.00013 (0.09519)	Loss 1.0935 (1.2789)	Acc 0.375 (0.573)
Epoch: [4][30/61]	Time 0.21632 (0.29853)	Data 0.00006 (0.09202)	Loss 0.7408 (1.2610)	Acc 0.875 (0.583)
Epoch: [4][31/61]	Time 0.21564 (0.29586)	Data 0.00005 (0.08905)	Loss 0.9265 (1.2502)	Acc 0.875 (0.593)
Epoch: [4][32/61]	Time 0.21585 (0.29336)	Data 0.00006 (0.08627)	Loss 0.8652 (1.2381)	Acc 0.625 (0.594)
Epoch: [4][33/61]	Time 0.21559 (0.29100)	Data 0.00013 (0.08366)	Loss 1.1548 (1.2356)	Acc 0.500 (0.591)
Epoch: [4][34/61]	Time 0.21555 (0.28878)	Data 0.00004 (0.08120)	Loss 1.6460 (1.2477)	Acc 0.500 (0.588)
Epoch: [4][35/61]	Time 0.21572 (0.28669)	Data 0.00004 (0.07888)	Loss 1.3515 (1.2507)	Acc 0.750 (0.593)
Epoch: [4][36/61]	Time 0.21553 (0.28472)	Data 0.00004 (0.07669)	Loss 1.4321 (1.2557)	Acc 0.500 (0.590)
Epoch: [4][37/61]	Time 0.21530 (0.28284)	Data 0.00004 (0.07462)	Loss 1.0055 (1.2489)	Acc 0.750 (0.595)
Epoch: [4][38/61]	Time 0.21627 (0.28109)	Data 0.00004 (0.07265)	Loss 1.0257 (1.2431)	Acc 0.625 (0.595)
Epoch: [4][39/61]	Time 0.21526 (0.27940)	Data 0.00004 (0.07079)	Loss 1.3311 (1.2453)	Acc 0.500 (0.593)
Epoch: [4][40/61]	Time 0.21617 (0.27782)	Data 0.00004 (0.06902)	Loss 1.1430 (1.2428)	Acc 0.750 (0.597)
Epoch: [4][41/61]	Time 0.21586 (0.27631)	Data 0.00004 (0.06734)	Loss 0.7474 (1.2307)	Acc 0.625 (0.598)
Epoch: [4][42/61]	Time 0.21576 (0.27487)	Data 0.00004 (0.06574)	Loss 1.4295 (1.2354)	Acc 0.500 (0.595)
Epoch: [4][43/61]	Time 0.21570 (0.27349)	Data 0.00004 (0.06421)	Loss 0.7374 (1.2238)	Acc 0.625 (0.596)
Epoch: [4][44/61]	Time 0.21689 (0.27221)	Data 0.00004 (0.06275)	Loss 1.5642 (1.2316)	Acc 0.375 (0.591)
Epoch: [4][45/61]	Time 0.21549 (0.27095)	Data 0.00006 (0.06136)	Loss 1.8525 (1.2454)	Acc 0.500 (0.589)
Epoch: [4][46/61]	Time 0.21531 (0.26974)	Data 0.00005 (0.06003)	Loss 0.4179 (1.2274)	Acc 0.875 (0.595)
Epoch: [4][47/61]	Time 0.21614 (0.26860)	Data 0.00003 (0.05875)	Loss 1.3801 (1.2306)	Acc 0.500 (0.593)
Epoch: [4][48/61]	Time 0.21550 (0.26749)	Data 0.00008 (0.05753)	Loss 1.2545 (1.2311)	Acc 0.625 (0.594)
Epoch: [4][49/61]	Time 0.21607 (0.26644)	Data 0.00008 (0.05636)	Loss 0.9448 (1.2253)	Acc 0.625 (0.594)
Epoch: [4][50/61]	Time 0.21573 (0.26543)	Data 0.00005 (0.05523)	Loss 1.0687 (1.2222)	Acc 0.500 (0.593)
Epoch: [4][51/61]	Time 0.21629 (0.26446)	Data 0.00004 (0.05415)	Loss 2.0307 (1.2380)	Acc 0.500 (0.591)
Epoch: [4][52/61]	Time 0.21566 (0.26352)	Data 0.00004 (0.05311)	Loss 1.1768 (1.2368)	Acc 0.500 (0.589)
Epoch: [4][53/61]	Time 0.21565 (0.26262)	Data 0.00004 (0.05211)	Loss 1.1798 (1.2358)	Acc 0.500 (0.587)
Epoch: [4][54/61]	Time 0.21596 (0.26176)	Data 0.00004 (0.05114)	Loss 1.1146 (1.2335)	Acc 0.500 (0.586)
Epoch: [4][55/61]	Time 0.21626 (0.26093)	Data 0.00004 (0.05021)	Loss 0.9253 (1.2279)	Acc 0.750 (0.589)
Epoch: [4][56/61]	Time 0.21537 (0.26012)	Data 0.00007 (0.04932)	Loss 1.4774 (1.2324)	Acc 0.375 (0.585)
Epoch: [4][57/61]	Time 0.21572 (0.25934)	Data 0.00006 (0.04845)	Loss 1.2617 (1.2329)	Acc 0.500 (0.583)
Epoch: [4][58/61]	Time 0.21592 (0.25859)	Data 0.00005 (0.04762)	Loss 1.1098 (1.2308)	Acc 0.500 (0.582)
Epoch: [4][59/61]	Time 0.21543 (0.25786)	Data 0.00005 (0.04681)	Loss 1.6325 (1.2376)	Acc 0.500 (0.581)
Epoch: [4][60/61]	Time 0.21622 (0.25716)	Data 0.00005 (0.04603)	Loss 1.0799 (1.2349)	Acc 0.625 (0.581)
Epoch: [4][61/61]	Time 0.07165 (0.25412)	Data 0.00002 (0.04528)	Loss 0.6509 (1.2325)	Acc 1.000 (0.583)
     Valid acc: 0.58298755186722  (0.5643153526970954, ep: 3)
train at epoch 5
Epoch: [5][0/132]	 lr: 0.01000	Time 3.889 (3.889)	Data 3.055 (3.055)	Loss 2.4671 (2.4671)	Acc 0.125 (0.125)	Precision 0.100(0.100)	Recall 0.050(0.050)
Epoch: [5][10/132]	 lr: 0.01000	Time 0.811 (1.091)	Data 0.000 (0.278)	Loss 1.7199 (1.5375)	Acc 0.375 (0.466)	Precision 0.273(0.340)	Recall 0.273(0.343)
Epoch: [5][20/132]	 lr: 0.01000	Time 0.935 (1.001)	Data 0.000 (0.146)	Loss 0.8762 (1.4812)	Acc 0.875 (0.500)	Precision 0.778(0.380)	Recall 0.778(0.372)
Epoch: [5][30/132]	 lr: 0.01000	Time 0.934 (0.979)	Data 0.000 (0.099)	Loss 1.5999 (1.5357)	Acc 0.500 (0.492)	Precision 0.364(0.374)	Recall 0.364(0.364)
Epoch: [5][40/132]	 lr: 0.01000	Time 0.934 (0.968)	Data 0.000 (0.075)	Loss 1.8947 (1.5154)	Acc 0.500 (0.518)	Precision 0.364(0.404)	Recall 0.364(0.392)
Epoch: [5][50/132]	 lr: 0.01000	Time 0.932 (0.961)	Data 0.000 (0.060)	Loss 1.0491 (1.5172)	Acc 0.500 (0.505)	Precision 0.444(0.391)	Recall 0.389(0.378)
Epoch: [5][60/132]	 lr: 0.01000	Time 0.936 (0.957)	Data 0.000 (0.050)	Loss 1.4250 (1.5548)	Acc 0.500 (0.498)	Precision 0.400(0.385)	Recall 0.350(0.368)
Epoch: [5][70/132]	 lr: 0.01000	Time 0.932 (0.954)	Data 0.000 (0.043)	Loss 1.1322 (1.5473)	Acc 0.625 (0.504)	Precision 0.400(0.388)	Recall 0.400(0.372)
Epoch: [5][80/132]	 lr: 0.01000	Time 0.931 (0.951)	Data 0.000 (0.038)	Loss 1.5558 (1.5626)	Acc 0.625 (0.502)	Precision 0.556(0.385)	Recall 0.444(0.367)
Epoch: [5][90/132]	 lr: 0.01000	Time 0.934 (0.949)	Data 0.000 (0.034)	Loss 1.8525 (1.5613)	Acc 0.500 (0.507)	Precision 0.300(0.388)	Recall 0.267(0.372)
Epoch: [5][100/132]	 lr: 0.01000	Time 0.935 (0.948)	Data 0.000 (0.030)	Loss 1.7141 (1.5507)	Acc 0.500 (0.509)	Precision 0.267(0.390)	Recall 0.300(0.373)
Epoch: [5][110/132]	 lr: 0.01000	Time 0.932 (0.947)	Data 0.000 (0.028)	Loss 1.0125 (1.5382)	Acc 0.750 (0.514)	Precision 0.600(0.395)	Recall 0.600(0.378)
Epoch: [5][120/132]	 lr: 0.01000	Time 0.933 (0.945)	Data 0.000 (0.025)	Loss 1.8405 (1.5355)	Acc 0.375 (0.517)	Precision 0.273(0.398)	Recall 0.273(0.381)
Epoch: [5][130/132]	 lr: 0.01000	Time 0.932 (0.944)	Data 0.000 (0.023)	Loss 1.5331 (1.5389)	Acc 0.250 (0.516)	Precision 0.154(0.400)	Recall 0.154(0.381)
validation at epoch 5
Epoch: [5][1/61]	Time 2.82732 (2.82732)	Data 2.61753 (2.61753)	Loss 2.2554 (2.2554)	Acc 0.375 (0.375)
Epoch: [5][2/61]	Time 0.19364 (1.51048)	Data 0.00010 (1.30881)	Loss 2.0731 (2.1642)	Acc 0.500 (0.438)
Epoch: [5][3/61]	Time 0.19059 (1.07051)	Data 0.00009 (0.87257)	Loss 1.4245 (1.9177)	Acc 0.625 (0.500)
Epoch: [5][4/61]	Time 0.18982 (0.85034)	Data 0.00009 (0.65445)	Loss 1.5176 (1.8176)	Acc 0.625 (0.531)
Epoch: [5][5/61]	Time 0.19049 (0.71837)	Data 0.00009 (0.52358)	Loss 2.2254 (1.8992)	Acc 0.375 (0.500)
Epoch: [5][6/61]	Time 0.19213 (0.63066)	Data 0.00012 (0.43634)	Loss 0.7914 (1.7146)	Acc 0.625 (0.521)
Epoch: [5][7/61]	Time 0.19372 (0.56824)	Data 0.00007 (0.37401)	Loss 0.7857 (1.5819)	Acc 0.875 (0.571)
Epoch: [5][8/61]	Time 0.19600 (0.52171)	Data 0.00009 (0.32727)	Loss 1.3824 (1.5569)	Acc 0.375 (0.547)
Epoch: [5][9/61]	Time 0.19647 (0.48557)	Data 0.00012 (0.29092)	Loss 1.9974 (1.6059)	Acc 0.375 (0.528)
Epoch: [5][10/61]	Time 0.19891 (0.45691)	Data 0.00009 (0.26184)	Loss 2.2930 (1.6746)	Acc 0.375 (0.512)
Epoch: [5][11/61]	Time 0.20097 (0.43364)	Data 0.00011 (0.23805)	Loss 1.9941 (1.7036)	Acc 0.375 (0.500)
Epoch: [5][12/61]	Time 0.20188 (0.41433)	Data 0.00009 (0.21822)	Loss 1.3077 (1.6706)	Acc 0.375 (0.490)
Epoch: [5][13/61]	Time 0.20426 (0.39817)	Data 0.00009 (0.20144)	Loss 2.3190 (1.7205)	Acc 0.500 (0.490)
Epoch: [5][14/61]	Time 0.20607 (0.38445)	Data 0.00009 (0.18706)	Loss 2.2651 (1.7594)	Acc 0.500 (0.491)
Epoch: [5][15/61]	Time 0.20883 (0.37274)	Data 0.00009 (0.17459)	Loss 0.4767 (1.6739)	Acc 0.875 (0.517)
Epoch: [5][16/61]	Time 0.20971 (0.36255)	Data 0.00009 (0.16368)	Loss 2.0791 (1.6992)	Acc 0.500 (0.516)
Epoch: [5][17/61]	Time 0.20926 (0.35353)	Data 0.00016 (0.15407)	Loss 1.2239 (1.6713)	Acc 0.625 (0.522)
Epoch: [5][18/61]	Time 0.20978 (0.34555)	Data 0.00011 (0.14551)	Loss 1.5303 (1.6634)	Acc 0.250 (0.507)
Epoch: [5][19/61]	Time 0.21297 (0.33857)	Data 0.00009 (0.13786)	Loss 0.5924 (1.6071)	Acc 0.750 (0.520)
Epoch: [5][20/61]	Time 0.21466 (0.33237)	Data 0.00015 (0.13097)	Loss 0.9933 (1.5764)	Acc 0.625 (0.525)
Epoch: [5][21/61]	Time 0.21660 (0.32686)	Data 0.00017 (0.12474)	Loss 0.9709 (1.5475)	Acc 0.750 (0.536)
Epoch: [5][22/61]	Time 0.21657 (0.32185)	Data 0.00011 (0.11908)	Loss 1.5802 (1.5490)	Acc 0.375 (0.528)
Epoch: [5][23/61]	Time 0.21591 (0.31724)	Data 0.00009 (0.11391)	Loss 3.0798 (1.6156)	Acc 0.250 (0.516)
Epoch: [5][24/61]	Time 0.21610 (0.31303)	Data 0.00009 (0.10916)	Loss 1.2500 (1.6003)	Acc 0.500 (0.516)
Epoch: [5][25/61]	Time 0.21635 (0.30916)	Data 0.00009 (0.10480)	Loss 0.9864 (1.5758)	Acc 0.500 (0.515)
Epoch: [5][26/61]	Time 0.21573 (0.30557)	Data 0.00009 (0.10077)	Loss 1.2137 (1.5619)	Acc 0.625 (0.519)
Epoch: [5][27/61]	Time 0.21632 (0.30226)	Data 0.00009 (0.09704)	Loss 0.9694 (1.5399)	Acc 0.625 (0.523)
Epoch: [5][28/61]	Time 0.21624 (0.29919)	Data 0.00009 (0.09358)	Loss 1.8529 (1.5511)	Acc 0.500 (0.522)
Epoch: [5][29/61]	Time 0.21590 (0.29632)	Data 0.00009 (0.09036)	Loss 1.2322 (1.5401)	Acc 0.375 (0.517)
Epoch: [5][30/61]	Time 0.21632 (0.29365)	Data 0.00006 (0.08735)	Loss 1.0337 (1.5232)	Acc 0.625 (0.521)
Epoch: [5][31/61]	Time 0.21612 (0.29115)	Data 0.00007 (0.08453)	Loss 1.8933 (1.5352)	Acc 0.500 (0.520)
Epoch: [5][32/61]	Time 0.21549 (0.28879)	Data 0.00005 (0.08189)	Loss 1.3329 (1.5288)	Acc 0.375 (0.516)
Epoch: [5][33/61]	Time 0.21564 (0.28657)	Data 0.00008 (0.07941)	Loss 1.3868 (1.5245)	Acc 0.625 (0.519)
Epoch: [5][34/61]	Time 0.21539 (0.28448)	Data 0.00006 (0.07708)	Loss 2.0204 (1.5391)	Acc 0.625 (0.522)
Epoch: [5][35/61]	Time 0.21581 (0.28251)	Data 0.00004 (0.07488)	Loss 1.6287 (1.5417)	Acc 0.625 (0.525)
Epoch: [5][36/61]	Time 0.21564 (0.28066)	Data 0.00004 (0.07280)	Loss 1.6700 (1.5452)	Acc 0.500 (0.524)
Epoch: [5][37/61]	Time 0.21637 (0.27892)	Data 0.00006 (0.07083)	Loss 1.4036 (1.5414)	Acc 0.750 (0.530)
Epoch: [5][38/61]	Time 0.21642 (0.27727)	Data 0.00006 (0.06897)	Loss 0.4637 (1.5131)	Acc 0.875 (0.539)
Epoch: [5][39/61]	Time 0.21589 (0.27570)	Data 0.00004 (0.06720)	Loss 1.7795 (1.5199)	Acc 0.500 (0.538)
Epoch: [5][40/61]	Time 0.21567 (0.27420)	Data 0.00004 (0.06552)	Loss 1.2008 (1.5119)	Acc 0.625 (0.541)
Epoch: [5][41/61]	Time 0.21600 (0.27278)	Data 0.00005 (0.06393)	Loss 1.4584 (1.5106)	Acc 0.250 (0.534)
Epoch: [5][42/61]	Time 0.21586 (0.27142)	Data 0.00008 (0.06241)	Loss 1.3630 (1.5071)	Acc 0.500 (0.533)
Epoch: [5][43/61]	Time 0.21601 (0.27014)	Data 0.00004 (0.06096)	Loss 2.1899 (1.5230)	Acc 0.375 (0.529)
Epoch: [5][44/61]	Time 0.21540 (0.26889)	Data 0.00004 (0.05957)	Loss 1.9365 (1.5324)	Acc 0.500 (0.528)
Epoch: [5][45/61]	Time 0.21673 (0.26773)	Data 0.00006 (0.05825)	Loss 2.8067 (1.5607)	Acc 0.250 (0.522)
Epoch: [5][46/61]	Time 0.21621 (0.26661)	Data 0.00005 (0.05698)	Loss 0.6081 (1.5400)	Acc 0.625 (0.524)
Epoch: [5][47/61]	Time 0.21519 (0.26552)	Data 0.00005 (0.05577)	Loss 1.6776 (1.5429)	Acc 0.375 (0.521)
Epoch: [5][48/61]	Time 0.21593 (0.26448)	Data 0.00006 (0.05461)	Loss 1.3747 (1.5394)	Acc 0.625 (0.523)
Epoch: [5][49/61]	Time 0.21547 (0.26348)	Data 0.00006 (0.05350)	Loss 1.7849 (1.5444)	Acc 0.500 (0.523)
Epoch: [5][50/61]	Time 0.21566 (0.26253)	Data 0.00004 (0.05243)	Loss 0.9153 (1.5318)	Acc 0.625 (0.525)
Epoch: [5][51/61]	Time 0.21561 (0.26161)	Data 0.00004 (0.05140)	Loss 2.4465 (1.5498)	Acc 0.250 (0.520)
Epoch: [5][52/61]	Time 0.21544 (0.26072)	Data 0.00006 (0.05041)	Loss 1.5383 (1.5495)	Acc 0.375 (0.517)
Epoch: [5][53/61]	Time 0.21542 (0.25987)	Data 0.00004 (0.04946)	Loss 0.9504 (1.5382)	Acc 0.500 (0.517)
Epoch: [5][54/61]	Time 0.21582 (0.25905)	Data 0.00008 (0.04855)	Loss 1.3664 (1.5351)	Acc 0.750 (0.521)
Epoch: [5][55/61]	Time 0.21575 (0.25826)	Data 0.00006 (0.04767)	Loss 0.7512 (1.5208)	Acc 0.750 (0.525)
Epoch: [5][56/61]	Time 0.21594 (0.25751)	Data 0.00005 (0.04682)	Loss 1.9892 (1.5292)	Acc 0.500 (0.525)
Epoch: [5][57/61]	Time 0.21537 (0.25677)	Data 0.00005 (0.04600)	Loss 2.1256 (1.5396)	Acc 0.500 (0.524)
Epoch: [5][58/61]	Time 0.21562 (0.25606)	Data 0.00005 (0.04521)	Loss 1.8682 (1.5453)	Acc 0.500 (0.524)
Epoch: [5][59/61]	Time 0.21542 (0.25537)	Data 0.00005 (0.04444)	Loss 1.8167 (1.5499)	Acc 0.625 (0.525)
Epoch: [5][60/61]	Time 0.21566 (0.25471)	Data 0.00005 (0.04370)	Loss 1.0323 (1.5413)	Acc 0.500 (0.525)
Epoch: [5][61/61]	Time 0.07149 (0.25170)	Data 0.00002 (0.04298)	Loss 0.2317 (1.5358)	Acc 1.000 (0.527)
     Valid acc: 0.5269709543568465  (0.58298755186722, ep: 4)
train at epoch 6
Epoch: [6][0/132]	 lr: 0.01000	Time 3.798 (3.798)	Data 2.969 (2.969)	Loss 1.2958 (1.2958)	Acc 0.500 (0.500)	Precision 0.400(0.400)	Recall 0.400(0.400)
Epoch: [6][10/132]	 lr: 0.01000	Time 0.883 (1.094)	Data 0.000 (0.270)	Loss 2.0974 (1.5924)	Acc 0.375 (0.489)	Precision 0.250(0.324)	Recall 0.250(0.343)
Epoch: [6][20/132]	 lr: 0.01000	Time 0.933 (1.016)	Data 0.000 (0.141)	Loss 0.8515 (1.5149)	Acc 0.500 (0.506)	Precision 0.364(0.352)	Recall 0.318(0.355)
Epoch: [6][30/132]	 lr: 0.01000	Time 0.934 (0.990)	Data 0.000 (0.096)	Loss 1.8794 (1.5127)	Acc 0.500 (0.528)	Precision 0.400(0.377)	Recall 0.400(0.377)
Epoch: [6][40/132]	 lr: 0.01000	Time 0.932 (0.976)	Data 0.000 (0.073)	Loss 1.5001 (1.5067)	Acc 0.375 (0.524)	Precision 0.250(0.380)	Recall 0.300(0.378)
Epoch: [6][50/132]	 lr: 0.01000	Time 0.937 (0.968)	Data 0.000 (0.058)	Loss 1.3664 (1.5026)	Acc 0.625 (0.534)	Precision 0.455(0.393)	Recall 0.455(0.389)
Epoch: [6][60/132]	 lr: 0.01000	Time 0.934 (0.962)	Data 0.000 (0.049)	Loss 1.9249 (1.5413)	Acc 0.375 (0.531)	Precision 0.250(0.391)	Recall 0.250(0.387)
Epoch: [6][70/132]	 lr: 0.01000	Time 0.931 (0.958)	Data 0.000 (0.042)	Loss 1.3131 (1.5454)	Acc 0.500 (0.525)	Precision 0.444(0.389)	Recall 0.389(0.381)
Epoch: [6][80/132]	 lr: 0.01000	Time 0.932 (0.955)	Data 0.000 (0.037)	Loss 1.2748 (1.5514)	Acc 0.375 (0.522)	Precision 0.167(0.385)	Recall 0.167(0.377)
Epoch: [6][90/132]	 lr: 0.01000	Time 0.924 (0.952)	Data 0.000 (0.033)	Loss 1.4007 (1.5186)	Acc 0.625 (0.537)	Precision 0.500(0.402)	Recall 0.450(0.392)
Epoch: [6][100/132]	 lr: 0.01000	Time 0.932 (0.950)	Data 0.000 (0.030)	Loss 1.1742 (1.5237)	Acc 0.625 (0.535)	Precision 0.556(0.400)	Recall 0.500(0.388)
Epoch: [6][110/132]	 lr: 0.01000	Time 0.934 (0.948)	Data 0.000 (0.027)	Loss 1.6698 (1.5155)	Acc 0.375 (0.538)	Precision 0.222(0.403)	Recall 0.167(0.391)
Epoch: [6][120/132]	 lr: 0.01000	Time 0.932 (0.947)	Data 0.000 (0.025)	Loss 0.8179 (1.5083)	Acc 0.750 (0.539)	Precision 0.556(0.406)	Recall 0.556(0.392)
Epoch: [6][130/132]	 lr: 0.01000	Time 0.933 (0.946)	Data 0.000 (0.023)	Loss 0.8620 (1.4830)	Acc 0.625 (0.542)	Precision 0.500(0.409)	Recall 0.500(0.396)
validation at epoch 6
Epoch: [6][1/61]	Time 2.82141 (2.82141)	Data 2.61292 (2.61292)	Loss 2.1801 (2.1801)	Acc 0.625 (0.625)
Epoch: [6][2/61]	Time 0.19358 (1.50750)	Data 0.00010 (1.30651)	Loss 1.6057 (1.8929)	Acc 0.500 (0.562)
Epoch: [6][3/61]	Time 0.18983 (1.06828)	Data 0.00012 (0.87105)	Loss 1.0596 (1.6151)	Acc 0.625 (0.583)
Epoch: [6][4/61]	Time 0.19022 (0.84876)	Data 0.00009 (0.65331)	Loss 0.9514 (1.4492)	Acc 0.625 (0.594)
Epoch: [6][5/61]	Time 0.19147 (0.71730)	Data 0.00010 (0.52267)	Loss 1.8550 (1.5303)	Acc 0.375 (0.550)
Epoch: [6][6/61]	Time 0.19393 (0.63007)	Data 0.00013 (0.43558)	Loss 0.2173 (1.3115)	Acc 0.875 (0.604)
Epoch: [6][7/61]	Time 0.19471 (0.56788)	Data 0.00007 (0.37336)	Loss 0.8290 (1.2426)	Acc 0.750 (0.625)
Epoch: [6][8/61]	Time 0.19723 (0.52155)	Data 0.00009 (0.32670)	Loss 0.5750 (1.1591)	Acc 0.875 (0.656)
Epoch: [6][9/61]	Time 0.19711 (0.48550)	Data 0.00010 (0.29041)	Loss 0.9055 (1.1309)	Acc 0.625 (0.653)
Epoch: [6][10/61]	Time 0.19829 (0.45678)	Data 0.00009 (0.26138)	Loss 1.9606 (1.2139)	Acc 0.500 (0.637)
Epoch: [6][11/61]	Time 0.20092 (0.43352)	Data 0.00009 (0.23763)	Loss 2.1414 (1.2982)	Acc 0.250 (0.602)
Epoch: [6][12/61]	Time 0.20022 (0.41408)	Data 0.00012 (0.21783)	Loss 1.3342 (1.3012)	Acc 0.625 (0.604)
Epoch: [6][13/61]	Time 0.20209 (0.39777)	Data 0.00008 (0.20108)	Loss 0.9622 (1.2751)	Acc 0.750 (0.615)
Epoch: [6][14/61]	Time 0.20453 (0.38397)	Data 0.00010 (0.18673)	Loss 1.6418 (1.3013)	Acc 0.500 (0.607)
Epoch: [6][15/61]	Time 0.20669 (0.37215)	Data 0.00011 (0.17429)	Loss 0.8544 (1.2715)	Acc 0.750 (0.617)
Epoch: [6][16/61]	Time 0.20661 (0.36180)	Data 0.00011 (0.16340)	Loss 1.8960 (1.3106)	Acc 0.375 (0.602)
Epoch: [6][17/61]	Time 0.20694 (0.35269)	Data 0.00023 (0.15380)	Loss 1.2166 (1.3050)	Acc 0.625 (0.603)
Epoch: [6][18/61]	Time 0.20600 (0.34454)	Data 0.00012 (0.14526)	Loss 0.6576 (1.2691)	Acc 0.750 (0.611)
Epoch: [6][19/61]	Time 0.20817 (0.33737)	Data 0.00009 (0.13762)	Loss 0.7633 (1.2425)	Acc 0.750 (0.618)
Epoch: [6][20/61]	Time 0.21004 (0.33100)	Data 0.00011 (0.13075)	Loss 1.0750 (1.2341)	Acc 0.625 (0.619)
Epoch: [6][21/61]	Time 0.21235 (0.32535)	Data 0.00009 (0.12453)	Loss 0.9836 (1.2221)	Acc 0.750 (0.625)
Epoch: [6][22/61]	Time 0.21199 (0.32020)	Data 0.00016 (0.11887)	Loss 0.6738 (1.1972)	Acc 0.875 (0.636)
Epoch: [6][23/61]	Time 0.21585 (0.31566)	Data 0.00008 (0.11371)	Loss 1.8339 (1.2249)	Acc 0.750 (0.641)
Epoch: [6][24/61]	Time 0.21626 (0.31152)	Data 0.00011 (0.10898)	Loss 1.0474 (1.2175)	Acc 0.500 (0.635)
Epoch: [6][25/61]	Time 0.21543 (0.30767)	Data 0.00010 (0.10462)	Loss 0.6644 (1.1954)	Acc 0.875 (0.645)
Epoch: [6][26/61]	Time 0.21609 (0.30415)	Data 0.00008 (0.10060)	Loss 1.0791 (1.1909)	Acc 0.750 (0.649)
Epoch: [6][27/61]	Time 0.21541 (0.30087)	Data 0.00009 (0.09688)	Loss 1.0292 (1.1849)	Acc 0.750 (0.653)
Epoch: [6][28/61]	Time 0.21597 (0.29783)	Data 0.00010 (0.09342)	Loss 1.3369 (1.1904)	Acc 0.625 (0.652)
Epoch: [6][29/61]	Time 0.21642 (0.29503)	Data 0.00009 (0.09020)	Loss 1.5815 (1.2038)	Acc 0.625 (0.651)
Epoch: [6][30/61]	Time 0.21552 (0.29238)	Data 0.00006 (0.08720)	Loss 1.2075 (1.2040)	Acc 0.750 (0.654)
Epoch: [6][31/61]	Time 0.21573 (0.28990)	Data 0.00006 (0.08439)	Loss 0.7314 (1.1887)	Acc 0.875 (0.661)
Epoch: [6][32/61]	Time 0.21545 (0.28758)	Data 0.00006 (0.08175)	Loss 0.5118 (1.1676)	Acc 0.875 (0.668)
Epoch: [6][33/61]	Time 0.21547 (0.28539)	Data 0.00010 (0.07928)	Loss 1.3279 (1.1724)	Acc 0.500 (0.663)
Epoch: [6][34/61]	Time 0.21530 (0.28333)	Data 0.00004 (0.07695)	Loss 1.2602 (1.1750)	Acc 0.625 (0.662)
Epoch: [6][35/61]	Time 0.21527 (0.28139)	Data 0.00004 (0.07475)	Loss 1.1061 (1.1730)	Acc 0.625 (0.661)
Epoch: [6][36/61]	Time 0.21529 (0.27955)	Data 0.00004 (0.07267)	Loss 1.3470 (1.1779)	Acc 0.625 (0.660)
Epoch: [6][37/61]	Time 0.21582 (0.27783)	Data 0.00003 (0.07071)	Loss 1.0473 (1.1743)	Acc 0.625 (0.659)
Epoch: [6][38/61]	Time 0.21541 (0.27618)	Data 0.00006 (0.06885)	Loss 1.0281 (1.1705)	Acc 0.750 (0.661)
Epoch: [6][39/61]	Time 0.21525 (0.27462)	Data 0.00004 (0.06709)	Loss 0.6461 (1.1570)	Acc 0.750 (0.663)
Epoch: [6][40/61]	Time 0.21547 (0.27314)	Data 0.00003 (0.06541)	Loss 1.0206 (1.1536)	Acc 0.625 (0.662)
Epoch: [6][41/61]	Time 0.21544 (0.27174)	Data 0.00004 (0.06382)	Loss 0.8541 (1.1463)	Acc 0.750 (0.665)
Epoch: [6][42/61]	Time 0.21537 (0.27039)	Data 0.00003 (0.06230)	Loss 0.5392 (1.1319)	Acc 0.750 (0.667)
Epoch: [6][43/61]	Time 0.21508 (0.26911)	Data 0.00004 (0.06085)	Loss 0.8942 (1.1263)	Acc 0.875 (0.672)
Epoch: [6][44/61]	Time 0.21514 (0.26788)	Data 0.00004 (0.05947)	Loss 1.5883 (1.1368)	Acc 0.500 (0.668)
Epoch: [6][45/61]	Time 0.21551 (0.26672)	Data 0.00007 (0.05815)	Loss 2.0011 (1.1561)	Acc 0.500 (0.664)
Epoch: [6][46/61]	Time 0.21554 (0.26560)	Data 0.00005 (0.05688)	Loss 0.1475 (1.1341)	Acc 1.000 (0.671)
Epoch: [6][47/61]	Time 0.21568 (0.26454)	Data 0.00005 (0.05568)	Loss 1.7074 (1.1463)	Acc 0.625 (0.670)
Epoch: [6][48/61]	Time 0.21526 (0.26352)	Data 0.00005 (0.05452)	Loss 1.4963 (1.1536)	Acc 0.750 (0.672)
Epoch: [6][49/61]	Time 0.21506 (0.26253)	Data 0.00007 (0.05341)	Loss 1.3923 (1.1585)	Acc 0.625 (0.671)
Epoch: [6][50/61]	Time 0.21533 (0.26158)	Data 0.00004 (0.05234)	Loss 0.7217 (1.1497)	Acc 0.625 (0.670)
Epoch: [6][51/61]	Time 0.21495 (0.26067)	Data 0.00004 (0.05131)	Loss 2.5792 (1.1778)	Acc 0.250 (0.662)
Epoch: [6][52/61]	Time 0.21538 (0.25980)	Data 0.00004 (0.05033)	Loss 1.3681 (1.1814)	Acc 0.500 (0.659)
Epoch: [6][53/61]	Time 0.21524 (0.25896)	Data 0.00004 (0.04938)	Loss 1.5844 (1.1890)	Acc 0.500 (0.656)
Epoch: [6][54/61]	Time 0.21535 (0.25815)	Data 0.00007 (0.04846)	Loss 0.9808 (1.1852)	Acc 0.625 (0.655)
Epoch: [6][55/61]	Time 0.21572 (0.25738)	Data 0.00006 (0.04758)	Loss 0.4095 (1.1711)	Acc 1.000 (0.661)
Epoch: [6][56/61]	Time 0.21489 (0.25662)	Data 0.00005 (0.04674)	Loss 1.2957 (1.1733)	Acc 0.375 (0.656)
Epoch: [6][57/61]	Time 0.21528 (0.25589)	Data 0.00007 (0.04592)	Loss 1.8381 (1.1850)	Acc 0.375 (0.651)
Epoch: [6][58/61]	Time 0.21511 (0.25519)	Data 0.00006 (0.04513)	Loss 1.0399 (1.1825)	Acc 0.750 (0.653)
Epoch: [6][59/61]	Time 0.21556 (0.25452)	Data 0.00005 (0.04436)	Loss 1.6671 (1.1907)	Acc 0.500 (0.650)
Epoch: [6][60/61]	Time 0.21542 (0.25387)	Data 0.00005 (0.04362)	Loss 1.3058 (1.1926)	Acc 0.625 (0.650)
Epoch: [6][61/61]	Time 0.07163 (0.25088)	Data 0.00002 (0.04291)	Loss 0.1221 (1.1882)	Acc 1.000 (0.651)
     Valid acc: 0.6514522821576764  (0.58298755186722, ep: 4)
train at epoch 7
Epoch: [7][0/132]	 lr: 0.01000	Time 3.695 (3.695)	Data 2.861 (2.861)	Loss 1.8662 (1.8662)	Acc 0.500 (0.500)	Precision 0.364(0.364)	Recall 0.318(0.318)
Epoch: [7][10/132]	 lr: 0.01000	Time 0.810 (1.073)	Data 0.000 (0.260)	Loss 0.3362 (1.3642)	Acc 1.000 (0.591)	Precision 1.000(0.478)	Recall 1.000(0.460)
Epoch: [7][20/132]	 lr: 0.01000	Time 0.925 (0.993)	Data 0.000 (0.136)	Loss 1.4494 (1.2695)	Acc 0.500 (0.625)	Precision 0.318(0.498)	Recall 0.364(0.491)
Epoch: [7][30/132]	 lr: 0.01000	Time 0.932 (0.973)	Data 0.000 (0.092)	Loss 0.9378 (1.2673)	Acc 0.750 (0.621)	Precision 0.667(0.503)	Recall 0.611(0.487)
Epoch: [7][40/132]	 lr: 0.01000	Time 0.931 (0.963)	Data 0.000 (0.070)	Loss 1.5525 (1.2892)	Acc 0.625 (0.619)	Precision 0.333(0.494)	Recall 0.333(0.479)
Epoch: [7][50/132]	 lr: 0.01000	Time 0.932 (0.957)	Data 0.000 (0.056)	Loss 1.5106 (1.3451)	Acc 0.625 (0.596)	Precision 0.450(0.472)	Recall 0.500(0.457)
Epoch: [7][60/132]	 lr: 0.01000	Time 0.933 (0.953)	Data 0.000 (0.047)	Loss 1.4073 (1.3514)	Acc 0.625 (0.605)	Precision 0.556(0.484)	Recall 0.500(0.464)
Epoch: [7][70/132]	 lr: 0.01000	Time 0.933 (0.950)	Data 0.000 (0.040)	Loss 2.1396 (1.4124)	Acc 0.500 (0.585)	Precision 0.333(0.465)	Recall 0.333(0.448)
Epoch: [7][80/132]	 lr: 0.01000	Time 0.934 (0.948)	Data 0.000 (0.035)	Loss 0.8112 (1.4485)	Acc 0.750 (0.566)	Precision 0.556(0.446)	Recall 0.556(0.430)
Epoch: [7][90/132]	 lr: 0.01000	Time 0.934 (0.946)	Data 0.000 (0.032)	Loss 1.2418 (1.4300)	Acc 0.625 (0.569)	Precision 0.455(0.446)	Recall 0.455(0.430)
Epoch: [7][100/132]	 lr: 0.01000	Time 0.932 (0.945)	Data 0.000 (0.028)	Loss 1.9623 (1.4274)	Acc 0.375 (0.573)	Precision 0.227(0.452)	Recall 0.273(0.436)
Epoch: [7][110/132]	 lr: 0.01000	Time 0.934 (0.944)	Data 0.000 (0.026)	Loss 1.0626 (1.4146)	Acc 0.625 (0.575)	Precision 0.455(0.454)	Recall 0.455(0.438)
Epoch: [7][120/132]	 lr: 0.01000	Time 0.932 (0.943)	Data 0.000 (0.024)	Loss 0.9339 (1.4025)	Acc 0.750 (0.576)	Precision 0.625(0.457)	Recall 0.562(0.439)
Epoch: [7][130/132]	 lr: 0.01000	Time 0.934 (0.942)	Data 0.000 (0.022)	Loss 1.3184 (1.4089)	Acc 0.375 (0.576)	Precision 0.273(0.458)	Recall 0.227(0.438)
validation at epoch 7
Epoch: [7][1/61]	Time 2.77661 (2.77661)	Data 2.56741 (2.56741)	Loss 2.5037 (2.5037)	Acc 0.500 (0.500)
Epoch: [7][2/61]	Time 0.19401 (1.48531)	Data 0.00010 (1.28376)	Loss 1.1248 (1.8142)	Acc 0.625 (0.562)
Epoch: [7][3/61]	Time 0.18994 (1.05352)	Data 0.00016 (0.85589)	Loss 1.2436 (1.6240)	Acc 0.500 (0.542)
Epoch: [7][4/61]	Time 0.19061 (0.83779)	Data 0.00007 (0.64194)	Loss 1.2012 (1.5183)	Acc 0.500 (0.531)
Epoch: [7][5/61]	Time 0.19183 (0.70860)	Data 0.00011 (0.51357)	Loss 1.3731 (1.4893)	Acc 0.625 (0.550)
Epoch: [7][6/61]	Time 0.19328 (0.62271)	Data 0.00009 (0.42799)	Loss 0.5280 (1.3291)	Acc 0.875 (0.604)
Epoch: [7][7/61]	Time 0.19537 (0.56166)	Data 0.00009 (0.36686)	Loss 0.3679 (1.1918)	Acc 0.875 (0.643)
Epoch: [7][8/61]	Time 0.19683 (0.51606)	Data 0.00009 (0.32102)	Loss 1.8793 (1.2777)	Acc 0.375 (0.609)
Epoch: [7][9/61]	Time 0.19832 (0.48076)	Data 0.00009 (0.28536)	Loss 1.1015 (1.2581)	Acc 0.500 (0.597)
Epoch: [7][10/61]	Time 0.20052 (0.45273)	Data 0.00009 (0.25683)	Loss 2.2285 (1.3552)	Acc 0.750 (0.613)
Epoch: [7][11/61]	Time 0.20285 (0.43002)	Data 0.00009 (0.23349)	Loss 2.7947 (1.4860)	Acc 0.125 (0.568)
Epoch: [7][12/61]	Time 0.20525 (0.41128)	Data 0.00011 (0.21404)	Loss 1.0704 (1.4514)	Acc 0.625 (0.573)
Epoch: [7][13/61]	Time 0.20705 (0.39557)	Data 0.00011 (0.19759)	Loss 1.5343 (1.4578)	Acc 0.625 (0.577)
Epoch: [7][14/61]	Time 0.20834 (0.38220)	Data 0.00011 (0.18348)	Loss 2.2093 (1.5114)	Acc 0.625 (0.580)
Epoch: [7][15/61]	Time 0.20969 (0.37070)	Data 0.00011 (0.17126)	Loss 0.5552 (1.4477)	Acc 0.875 (0.600)
Epoch: [7][16/61]	Time 0.21132 (0.36074)	Data 0.00009 (0.16056)	Loss 1.1626 (1.4299)	Acc 0.625 (0.602)
Epoch: [7][17/61]	Time 0.21367 (0.35209)	Data 0.00015 (0.15112)	Loss 0.7690 (1.3910)	Acc 0.750 (0.610)
Epoch: [7][18/61]	Time 0.21432 (0.34443)	Data 0.00012 (0.14273)	Loss 0.8441 (1.3606)	Acc 0.750 (0.618)
Epoch: [7][19/61]	Time 0.21453 (0.33760)	Data 0.00016 (0.13523)	Loss 1.1359 (1.3488)	Acc 0.500 (0.612)
Epoch: [7][20/61]	Time 0.21377 (0.33141)	Data 0.00011 (0.12847)	Loss 0.7792 (1.3203)	Acc 0.750 (0.619)
Epoch: [7][21/61]	Time 0.21416 (0.32582)	Data 0.00009 (0.12236)	Loss 0.9289 (1.3017)	Acc 0.750 (0.625)
Epoch: [7][22/61]	Time 0.21662 (0.32086)	Data 0.00009 (0.11680)	Loss 1.5698 (1.3139)	Acc 0.500 (0.619)
Epoch: [7][23/61]	Time 0.21745 (0.31636)	Data 0.00012 (0.11173)	Loss 2.5638 (1.3682)	Acc 0.500 (0.614)
Epoch: [7][24/61]	Time 0.21647 (0.31220)	Data 0.00011 (0.10708)	Loss 1.4202 (1.3704)	Acc 0.750 (0.620)
Epoch: [7][25/61]	Time 0.21554 (0.30833)	Data 0.00012 (0.10280)	Loss 0.6821 (1.3428)	Acc 0.625 (0.620)
Epoch: [7][26/61]	Time 0.21686 (0.30482)	Data 0.00009 (0.09885)	Loss 0.8475 (1.3238)	Acc 0.625 (0.620)
Epoch: [7][27/61]	Time 0.21630 (0.30154)	Data 0.00015 (0.09519)	Loss 0.4635 (1.2919)	Acc 0.750 (0.625)
Epoch: [7][28/61]	Time 0.21563 (0.29847)	Data 0.00008 (0.09180)	Loss 1.7184 (1.3072)	Acc 0.625 (0.625)
Epoch: [7][29/61]	Time 0.21586 (0.29562)	Data 0.00009 (0.08864)	Loss 1.5661 (1.3161)	Acc 0.500 (0.621)
Epoch: [7][30/61]	Time 0.21594 (0.29297)	Data 0.00006 (0.08568)	Loss 0.7002 (1.2956)	Acc 0.500 (0.617)
Epoch: [7][31/61]	Time 0.21586 (0.29048)	Data 0.00006 (0.08292)	Loss 0.8716 (1.2819)	Acc 0.750 (0.621)
Epoch: [7][32/61]	Time 0.21579 (0.28814)	Data 0.00005 (0.08033)	Loss 1.3227 (1.2832)	Acc 0.625 (0.621)
Epoch: [7][33/61]	Time 0.21580 (0.28595)	Data 0.00010 (0.07790)	Loss 1.2663 (1.2826)	Acc 0.625 (0.621)
Epoch: [7][34/61]	Time 0.21534 (0.28387)	Data 0.00004 (0.07561)	Loss 1.0914 (1.2770)	Acc 0.625 (0.621)
Epoch: [7][35/61]	Time 0.21533 (0.28192)	Data 0.00005 (0.07345)	Loss 0.5145 (1.2552)	Acc 0.875 (0.629)
Epoch: [7][36/61]	Time 0.21511 (0.28006)	Data 0.00004 (0.07141)	Loss 1.7683 (1.2695)	Acc 0.500 (0.625)
Epoch: [7][37/61]	Time 0.21528 (0.27831)	Data 0.00004 (0.06948)	Loss 0.7659 (1.2559)	Acc 0.750 (0.628)
Epoch: [7][38/61]	Time 0.21565 (0.27666)	Data 0.00004 (0.06765)	Loss 0.7333 (1.2421)	Acc 0.875 (0.635)
Epoch: [7][39/61]	Time 0.21554 (0.27509)	Data 0.00006 (0.06592)	Loss 0.6874 (1.2279)	Acc 0.625 (0.635)
Epoch: [7][40/61]	Time 0.21568 (0.27361)	Data 0.00003 (0.06427)	Loss 0.8533 (1.2185)	Acc 0.750 (0.637)
Epoch: [7][41/61]	Time 0.21533 (0.27219)	Data 0.00004 (0.06271)	Loss 1.4480 (1.2241)	Acc 0.625 (0.637)
Epoch: [7][42/61]	Time 0.21584 (0.27085)	Data 0.00004 (0.06122)	Loss 0.6937 (1.2115)	Acc 0.750 (0.640)
Epoch: [7][43/61]	Time 0.21587 (0.26957)	Data 0.00007 (0.05979)	Loss 1.5920 (1.2204)	Acc 0.750 (0.642)
Epoch: [7][44/61]	Time 0.21531 (0.26833)	Data 0.00003 (0.05844)	Loss 1.0229 (1.2159)	Acc 0.625 (0.642)
Epoch: [7][45/61]	Time 0.21565 (0.26716)	Data 0.00005 (0.05714)	Loss 2.1111 (1.2358)	Acc 0.250 (0.633)
Epoch: [7][46/61]	Time 0.21572 (0.26604)	Data 0.00005 (0.05590)	Loss 0.0988 (1.2110)	Acc 1.000 (0.641)
Epoch: [7][47/61]	Time 0.21562 (0.26497)	Data 0.00005 (0.05471)	Loss 1.3420 (1.2138)	Acc 0.375 (0.636)
Epoch: [7][48/61]	Time 0.21566 (0.26394)	Data 0.00007 (0.05357)	Loss 0.7621 (1.2044)	Acc 0.500 (0.633)
Epoch: [7][49/61]	Time 0.21540 (0.26295)	Data 0.00008 (0.05248)	Loss 0.9317 (1.1988)	Acc 0.625 (0.633)
Epoch: [7][50/61]	Time 0.21551 (0.26200)	Data 0.00004 (0.05143)	Loss 1.0881 (1.1966)	Acc 0.500 (0.630)
Epoch: [7][51/61]	Time 0.21575 (0.26110)	Data 0.00004 (0.05042)	Loss 1.8697 (1.2098)	Acc 0.500 (0.627)
Epoch: [7][52/61]	Time 0.21561 (0.26022)	Data 0.00004 (0.04945)	Loss 1.8511 (1.2222)	Acc 0.500 (0.625)
Epoch: [7][53/61]	Time 0.21530 (0.25938)	Data 0.00004 (0.04852)	Loss 1.0220 (1.2184)	Acc 0.625 (0.625)
Epoch: [7][54/61]	Time 0.21558 (0.25856)	Data 0.00004 (0.04762)	Loss 0.8639 (1.2118)	Acc 0.875 (0.630)
Epoch: [7][55/61]	Time 0.21572 (0.25779)	Data 0.00006 (0.04676)	Loss 1.5387 (1.2178)	Acc 0.500 (0.627)
Epoch: [7][56/61]	Time 0.21588 (0.25704)	Data 0.00006 (0.04592)	Loss 1.8987 (1.2299)	Acc 0.375 (0.623)
Epoch: [7][57/61]	Time 0.21543 (0.25631)	Data 0.00006 (0.04512)	Loss 2.1482 (1.2460)	Acc 0.625 (0.623)
Epoch: [7][58/61]	Time 0.21515 (0.25560)	Data 0.00004 (0.04434)	Loss 0.6080 (1.2350)	Acc 0.750 (0.625)
Epoch: [7][59/61]	Time 0.21544 (0.25492)	Data 0.00006 (0.04359)	Loss 1.4331 (1.2384)	Acc 0.625 (0.625)
Epoch: [7][60/61]	Time 0.21635 (0.25427)	Data 0.00005 (0.04287)	Loss 1.6305 (1.2449)	Acc 0.500 (0.623)
Epoch: [7][61/61]	Time 0.07165 (0.25128)	Data 0.00002 (0.04216)	Loss 0.2847 (1.2409)	Acc 1.000 (0.624)
     Valid acc: 0.6244813278008299  (0.6514522821576764, ep: 6)
train at epoch 8
Epoch: [8][0/132]	 lr: 0.01000	Time 4.115 (4.115)	Data 3.274 (3.274)	Loss 1.2342 (1.2342)	Acc 0.750 (0.750)	Precision 0.667(0.667)	Recall 0.611(0.611)
Epoch: [8][10/132]	 lr: 0.01000	Time 0.887 (1.124)	Data 0.000 (0.298)	Loss 1.0652 (1.4871)	Acc 0.625 (0.534)	Precision 0.500(0.412)	Recall 0.500(0.390)
Epoch: [8][20/132]	 lr: 0.01000	Time 0.934 (1.032)	Data 0.000 (0.156)	Loss 2.2323 (1.5274)	Acc 0.500 (0.530)	Precision 0.400(0.417)	Recall 0.350(0.385)
Epoch: [8][30/132]	 lr: 0.01000	Time 0.933 (1.000)	Data 0.000 (0.106)	Loss 1.2156 (1.5189)	Acc 0.750 (0.540)	Precision 0.556(0.420)	Recall 0.556(0.391)
Epoch: [8][40/132]	 lr: 0.01000	Time 0.931 (0.984)	Data 0.000 (0.080)	Loss 1.7449 (1.4545)	Acc 0.375 (0.561)	Precision 0.300(0.444)	Recall 0.250(0.418)
Epoch: [8][50/132]	 lr: 0.01000	Time 0.933 (0.974)	Data 0.000 (0.064)	Loss 0.9099 (1.4040)	Acc 0.750 (0.569)	Precision 0.571(0.447)	Recall 0.571(0.424)
Epoch: [8][60/132]	 lr: 0.01000	Time 0.935 (0.967)	Data 0.000 (0.054)	Loss 1.2053 (1.4054)	Acc 0.625 (0.561)	Precision 0.500(0.440)	Recall 0.500(0.418)
Epoch: [8][70/132]	 lr: 0.01000	Time 0.932 (0.962)	Data 0.000 (0.046)	Loss 1.1943 (1.3908)	Acc 0.625 (0.567)	Precision 0.450(0.446)	Recall 0.500(0.425)
Epoch: [8][80/132]	 lr: 0.01000	Time 0.934 (0.959)	Data 0.000 (0.041)	Loss 0.8514 (1.3973)	Acc 0.750 (0.560)	Precision 0.600(0.442)	Recall 0.600(0.422)
Epoch: [8][90/132]	 lr: 0.01000	Time 0.925 (0.955)	Data 0.000 (0.036)	Loss 0.9066 (1.3962)	Acc 0.875 (0.562)	Precision 0.714(0.440)	Recall 0.714(0.421)
Epoch: [8][100/132]	 lr: 0.01000	Time 0.933 (0.953)	Data 0.000 (0.033)	Loss 1.0172 (1.3891)	Acc 0.625 (0.564)	Precision 0.625(0.440)	Recall 0.562(0.423)
Epoch: [8][110/132]	 lr: 0.01000	Time 0.931 (0.951)	Data 0.000 (0.030)	Loss 0.9726 (1.3824)	Acc 0.750 (0.569)	Precision 0.625(0.445)	Recall 0.562(0.426)
Epoch: [8][120/132]	 lr: 0.01000	Time 0.931 (0.949)	Data 0.000 (0.027)	Loss 1.3821 (1.3827)	Acc 0.500 (0.568)	Precision 0.364(0.444)	Recall 0.364(0.425)
Epoch: [8][130/132]	 lr: 0.01000	Time 0.934 (0.947)	Data 0.000 (0.025)	Loss 1.2735 (1.3756)	Acc 0.500 (0.565)	Precision 0.364(0.439)	Recall 0.364(0.422)
validation at epoch 8
Epoch: [8][1/61]	Time 2.81391 (2.81391)	Data 2.59868 (2.59868)	Loss 1.6099 (1.6099)	Acc 0.500 (0.500)
Epoch: [8][2/61]	Time 0.19581 (1.50486)	Data 0.00011 (1.29939)	Loss 0.5165 (1.0632)	Acc 0.750 (0.625)
Epoch: [8][3/61]	Time 0.18973 (1.06648)	Data 0.00010 (0.86630)	Loss 0.5421 (0.8895)	Acc 0.875 (0.708)
Epoch: [8][4/61]	Time 0.19098 (0.84761)	Data 0.00013 (0.64976)	Loss 0.8669 (0.8838)	Acc 0.625 (0.688)
Epoch: [8][5/61]	Time 0.19139 (0.71636)	Data 0.00009 (0.51982)	Loss 0.7228 (0.8516)	Acc 0.875 (0.725)
Epoch: [8][6/61]	Time 0.19217 (0.62900)	Data 0.00011 (0.43321)	Loss 0.5209 (0.7965)	Acc 0.875 (0.750)
Epoch: [8][7/61]	Time 0.19429 (0.56690)	Data 0.00011 (0.37133)	Loss 0.6966 (0.7822)	Acc 0.750 (0.750)
Epoch: [8][8/61]	Time 0.19556 (0.52048)	Data 0.00011 (0.32493)	Loss 1.0224 (0.8123)	Acc 0.750 (0.750)
Epoch: [8][9/61]	Time 0.19699 (0.48454)	Data 0.00009 (0.28884)	Loss 0.8882 (0.8207)	Acc 0.625 (0.736)
Epoch: [8][10/61]	Time 0.19917 (0.45600)	Data 0.00009 (0.25996)	Loss 2.0131 (0.9399)	Acc 0.625 (0.725)
Epoch: [8][11/61]	Time 0.20155 (0.43287)	Data 0.00009 (0.23634)	Loss 1.8927 (1.0265)	Acc 0.500 (0.705)
Epoch: [8][12/61]	Time 0.20268 (0.41369)	Data 0.00012 (0.21665)	Loss 0.8059 (1.0082)	Acc 0.750 (0.708)
Epoch: [8][13/61]	Time 0.20510 (0.39764)	Data 0.00009 (0.19999)	Loss 0.8096 (0.9929)	Acc 0.625 (0.702)
Epoch: [8][14/61]	Time 0.20649 (0.38399)	Data 0.00012 (0.18572)	Loss 2.0303 (1.0670)	Acc 0.500 (0.688)
Epoch: [8][15/61]	Time 0.20862 (0.37230)	Data 0.00010 (0.17334)	Loss 1.4591 (1.0931)	Acc 0.500 (0.675)
Epoch: [8][16/61]	Time 0.20899 (0.36209)	Data 0.00012 (0.16252)	Loss 1.0904 (1.0930)	Acc 0.625 (0.672)
Epoch: [8][17/61]	Time 0.21113 (0.35321)	Data 0.00019 (0.15297)	Loss 1.3532 (1.1083)	Acc 0.500 (0.662)
Epoch: [8][18/61]	Time 0.21291 (0.34541)	Data 0.00019 (0.14448)	Loss 0.6720 (1.0840)	Acc 0.625 (0.660)
Epoch: [8][19/61]	Time 0.21387 (0.33849)	Data 0.00010 (0.13688)	Loss 0.7449 (1.0662)	Acc 0.750 (0.664)
Epoch: [8][20/61]	Time 0.21558 (0.33235)	Data 0.00016 (0.13004)	Loss 0.6869 (1.0472)	Acc 0.750 (0.669)
Epoch: [8][21/61]	Time 0.21671 (0.32684)	Data 0.00010 (0.12386)	Loss 0.7562 (1.0334)	Acc 0.750 (0.673)
Epoch: [8][22/61]	Time 0.21732 (0.32186)	Data 0.00012 (0.11823)	Loss 0.8073 (1.0231)	Acc 0.750 (0.676)
Epoch: [8][23/61]	Time 0.21614 (0.31726)	Data 0.00010 (0.11310)	Loss 1.0217 (1.0230)	Acc 0.625 (0.674)
Epoch: [8][24/61]	Time 0.21648 (0.31307)	Data 0.00009 (0.10839)	Loss 1.2169 (1.0311)	Acc 0.625 (0.672)
Epoch: [8][25/61]	Time 0.21653 (0.30920)	Data 0.00009 (0.10406)	Loss 1.4179 (1.0466)	Acc 0.375 (0.660)
Epoch: [8][26/61]	Time 0.21697 (0.30566)	Data 0.00008 (0.10006)	Loss 0.8017 (1.0372)	Acc 0.750 (0.663)
Epoch: [8][27/61]	Time 0.21733 (0.30239)	Data 0.00013 (0.09636)	Loss 0.5070 (1.0175)	Acc 0.875 (0.671)
Epoch: [8][28/61]	Time 0.21593 (0.29930)	Data 0.00008 (0.09292)	Loss 1.0523 (1.0188)	Acc 0.500 (0.665)
Epoch: [8][29/61]	Time 0.21660 (0.29645)	Data 0.00008 (0.08972)	Loss 1.6766 (1.0414)	Acc 0.500 (0.659)
Epoch: [8][30/61]	Time 0.21631 (0.29377)	Data 0.00006 (0.08673)	Loss 1.0725 (1.0425)	Acc 0.750 (0.662)
Epoch: [8][31/61]	Time 0.21628 (0.29127)	Data 0.00006 (0.08393)	Loss 1.2758 (1.0500)	Acc 0.625 (0.661)
Epoch: [8][32/61]	Time 0.21564 (0.28891)	Data 0.00005 (0.08131)	Loss 0.7840 (1.0417)	Acc 0.625 (0.660)
Epoch: [8][33/61]	Time 0.21557 (0.28669)	Data 0.00009 (0.07885)	Loss 1.1918 (1.0462)	Acc 0.625 (0.659)
Epoch: [8][34/61]	Time 0.21707 (0.28464)	Data 0.00008 (0.07653)	Loss 0.4086 (1.0275)	Acc 1.000 (0.669)
Epoch: [8][35/61]	Time 0.21609 (0.28268)	Data 0.00005 (0.07435)	Loss 0.9655 (1.0257)	Acc 0.625 (0.668)
Epoch: [8][36/61]	Time 0.21568 (0.28082)	Data 0.00007 (0.07228)	Loss 1.1867 (1.0302)	Acc 0.625 (0.667)
Epoch: [8][37/61]	Time 0.21575 (0.27906)	Data 0.00004 (0.07033)	Loss 1.0927 (1.0319)	Acc 0.625 (0.666)
Epoch: [8][38/61]	Time 0.21659 (0.27742)	Data 0.00004 (0.06848)	Loss 1.3949 (1.0414)	Acc 0.500 (0.661)
Epoch: [8][39/61]	Time 0.21657 (0.27586)	Data 0.00004 (0.06673)	Loss 0.4074 (1.0252)	Acc 0.875 (0.667)
Epoch: [8][40/61]	Time 0.21556 (0.27435)	Data 0.00003 (0.06506)	Loss 0.5705 (1.0138)	Acc 0.875 (0.672)
Epoch: [8][41/61]	Time 0.21594 (0.27293)	Data 0.00004 (0.06347)	Loss 0.9747 (1.0129)	Acc 0.625 (0.671)
Epoch: [8][42/61]	Time 0.21622 (0.27158)	Data 0.00004 (0.06196)	Loss 0.7399 (1.0064)	Acc 0.625 (0.670)
Epoch: [8][43/61]	Time 0.21543 (0.27027)	Data 0.00004 (0.06052)	Loss 0.9408 (1.0048)	Acc 0.500 (0.666)
Epoch: [8][44/61]	Time 0.21593 (0.26904)	Data 0.00004 (0.05915)	Loss 1.1075 (1.0072)	Acc 0.625 (0.665)
Epoch: [8][45/61]	Time 0.21668 (0.26787)	Data 0.00006 (0.05784)	Loss 2.2739 (1.0353)	Acc 0.500 (0.661)
Epoch: [8][46/61]	Time 0.21504 (0.26672)	Data 0.00006 (0.05658)	Loss 0.2424 (1.0181)	Acc 1.000 (0.668)
Epoch: [8][47/61]	Time 0.21554 (0.26563)	Data 0.00005 (0.05538)	Loss 1.1532 (1.0210)	Acc 0.375 (0.662)
Epoch: [8][48/61]	Time 0.21593 (0.26460)	Data 0.00005 (0.05422)	Loss 0.7343 (1.0150)	Acc 0.875 (0.667)
Epoch: [8][49/61]	Time 0.21589 (0.26360)	Data 0.00006 (0.05312)	Loss 0.7835 (1.0103)	Acc 0.625 (0.666)
Epoch: [8][50/61]	Time 0.21583 (0.26265)	Data 0.00005 (0.05206)	Loss 0.9464 (1.0090)	Acc 0.500 (0.662)
Epoch: [8][51/61]	Time 0.21572 (0.26173)	Data 0.00004 (0.05104)	Loss 1.9436 (1.0273)	Acc 0.500 (0.659)
Epoch: [8][52/61]	Time 0.21599 (0.26085)	Data 0.00006 (0.05006)	Loss 0.9070 (1.0250)	Acc 0.625 (0.659)
Epoch: [8][53/61]	Time 0.21599 (0.26000)	Data 0.00004 (0.04911)	Loss 1.2611 (1.0294)	Acc 0.750 (0.660)
Epoch: [8][54/61]	Time 0.21602 (0.25919)	Data 0.00005 (0.04820)	Loss 0.5184 (1.0200)	Acc 0.875 (0.664)
Epoch: [8][55/61]	Time 0.21580 (0.25840)	Data 0.00005 (0.04733)	Loss 0.4730 (1.0100)	Acc 0.875 (0.668)
Epoch: [8][56/61]	Time 0.21579 (0.25764)	Data 0.00005 (0.04648)	Loss 1.6830 (1.0221)	Acc 0.375 (0.663)
Epoch: [8][57/61]	Time 0.21666 (0.25692)	Data 0.00005 (0.04567)	Loss 0.9687 (1.0211)	Acc 0.625 (0.662)
Epoch: [8][58/61]	Time 0.21620 (0.25622)	Data 0.00006 (0.04488)	Loss 0.4636 (1.0115)	Acc 0.875 (0.666)
Epoch: [8][59/61]	Time 0.21656 (0.25555)	Data 0.00008 (0.04412)	Loss 1.4354 (1.0187)	Acc 0.500 (0.663)
Epoch: [8][60/61]	Time 0.21612 (0.25489)	Data 0.00006 (0.04339)	Loss 2.2287 (1.0389)	Acc 0.500 (0.660)
Epoch: [8][61/61]	Time 0.07162 (0.25188)	Data 0.00002 (0.04268)	Loss 0.9002 (1.0383)	Acc 0.500 (0.660)
     Valid acc: 0.6597510373443983  (0.6514522821576764, ep: 6)
train at epoch 9
Epoch: [9][0/132]	 lr: 0.01000	Time 4.134 (4.134)	Data 3.309 (3.309)	Loss 1.9260 (1.9260)	Acc 0.500 (0.500)	Precision 0.364(0.364)	Recall 0.364(0.364)
Epoch: [9][10/132]	 lr: 0.01000	Time 0.815 (1.113)	Data 0.000 (0.301)	Loss 0.9228 (1.4606)	Acc 0.875 (0.591)	Precision 0.778(0.460)	Recall 0.778(0.446)
Epoch: [9][20/132]	 lr: 0.01000	Time 0.934 (1.012)	Data 0.000 (0.158)	Loss 1.0076 (1.3882)	Acc 0.750 (0.571)	Precision 0.688(0.435)	Recall 0.688(0.418)
Epoch: [9][30/132]	 lr: 0.01000	Time 0.936 (0.987)	Data 0.000 (0.107)	Loss 1.4695 (1.3913)	Acc 0.625 (0.581)	Precision 0.500(0.449)	Recall 0.450(0.429)
Epoch: [9][40/132]	 lr: 0.01000	Time 0.935 (0.975)	Data 0.000 (0.081)	Loss 2.0214 (1.4700)	Acc 0.375 (0.552)	Precision 0.273(0.416)	Recall 0.182(0.400)
Epoch: [9][50/132]	 lr: 0.01000	Time 0.935 (0.967)	Data 0.000 (0.065)	Loss 1.3854 (1.4476)	Acc 0.625 (0.564)	Precision 0.455(0.430)	Recall 0.455(0.417)
Epoch: [9][60/132]	 lr: 0.01000	Time 0.936 (0.961)	Data 0.000 (0.054)	Loss 1.7777 (1.4435)	Acc 0.500 (0.561)	Precision 0.444(0.427)	Recall 0.333(0.414)
Epoch: [9][70/132]	 lr: 0.01000	Time 0.934 (0.957)	Data 0.000 (0.047)	Loss 0.8428 (1.4317)	Acc 0.875 (0.572)	Precision 0.750(0.445)	Recall 0.750(0.430)
Epoch: [9][80/132]	 lr: 0.01000	Time 0.932 (0.955)	Data 0.000 (0.041)	Loss 2.2326 (1.4178)	Acc 0.375 (0.579)	Precision 0.250(0.449)	Recall 0.250(0.432)
Epoch: [9][90/132]	 lr: 0.01000	Time 0.935 (0.952)	Data 0.000 (0.037)	Loss 1.7404 (1.4160)	Acc 0.500 (0.581)	Precision 0.400(0.456)	Recall 0.300(0.436)
Epoch: [9][100/132]	 lr: 0.01000	Time 0.932 (0.950)	Data 0.000 (0.033)	Loss 1.0352 (1.3840)	Acc 0.750 (0.592)	Precision 0.556(0.466)	Recall 0.556(0.447)
Epoch: [9][110/132]	 lr: 0.01000	Time 0.932 (0.949)	Data 0.000 (0.030)	Loss 1.5742 (1.3536)	Acc 0.625 (0.604)	Precision 0.562(0.482)	Recall 0.500(0.460)
Epoch: [9][120/132]	 lr: 0.01000	Time 0.932 (0.947)	Data 0.000 (0.027)	Loss 0.8262 (1.3349)	Acc 0.875 (0.610)	Precision 0.750(0.485)	Recall 0.750(0.463)
Epoch: [9][130/132]	 lr: 0.01000	Time 0.932 (0.946)	Data 0.000 (0.025)	Loss 1.5480 (1.3476)	Acc 0.500 (0.605)	Precision 0.333(0.482)	Recall 0.333(0.461)
validation at epoch 9
Epoch: [9][1/61]	Time 2.72700 (2.72700)	Data 2.51041 (2.51041)	Loss 1.8406 (1.8406)	Acc 0.500 (0.500)
Epoch: [9][2/61]	Time 0.19712 (1.46206)	Data 0.00014 (1.25528)	Loss 0.8510 (1.3458)	Acc 0.625 (0.562)
Epoch: [9][3/61]	Time 0.18994 (1.03802)	Data 0.00016 (0.83691)	Loss 1.2876 (1.3264)	Acc 0.625 (0.583)
Epoch: [9][4/61]	Time 0.18989 (0.82599)	Data 0.00014 (0.62771)	Loss 1.4711 (1.3626)	Acc 0.625 (0.594)
Epoch: [9][5/61]	Time 0.19137 (0.69906)	Data 0.00011 (0.50219)	Loss 0.3619 (1.1624)	Acc 1.000 (0.675)
Epoch: [9][6/61]	Time 0.19280 (0.61469)	Data 0.00010 (0.41851)	Loss 0.3755 (1.0313)	Acc 0.750 (0.688)
Epoch: [9][7/61]	Time 0.19432 (0.55463)	Data 0.00009 (0.35874)	Loss 0.4290 (0.9453)	Acc 0.875 (0.714)
Epoch: [9][8/61]	Time 0.19643 (0.50986)	Data 0.00009 (0.31391)	Loss 0.9957 (0.9516)	Acc 0.750 (0.719)
Epoch: [9][9/61]	Time 0.19778 (0.47518)	Data 0.00013 (0.27904)	Loss 0.3251 (0.8820)	Acc 1.000 (0.750)
Epoch: [9][10/61]	Time 0.20023 (0.44769)	Data 0.00011 (0.25115)	Loss 1.7178 (0.9655)	Acc 0.625 (0.738)
Epoch: [9][11/61]	Time 0.20129 (0.42529)	Data 0.00012 (0.22833)	Loss 2.0659 (1.0656)	Acc 0.625 (0.727)
Epoch: [9][12/61]	Time 0.20252 (0.40672)	Data 0.00011 (0.20931)	Loss 0.6161 (1.0281)	Acc 0.875 (0.740)
Epoch: [9][13/61]	Time 0.20551 (0.39125)	Data 0.00008 (0.19322)	Loss 0.7570 (1.0073)	Acc 0.750 (0.740)
Epoch: [9][14/61]	Time 0.20699 (0.37809)	Data 0.00011 (0.17942)	Loss 1.7647 (1.0614)	Acc 0.500 (0.723)
Epoch: [9][15/61]	Time 0.20905 (0.36682)	Data 0.00011 (0.16747)	Loss 1.3325 (1.0794)	Acc 0.625 (0.717)
Epoch: [9][16/61]	Time 0.21095 (0.35707)	Data 0.00011 (0.15701)	Loss 2.0088 (1.1375)	Acc 0.375 (0.695)
Epoch: [9][17/61]	Time 0.21302 (0.34860)	Data 0.00013 (0.14778)	Loss 0.6409 (1.1083)	Acc 0.750 (0.699)
Epoch: [9][18/61]	Time 0.21563 (0.34121)	Data 0.00009 (0.13958)	Loss 0.7629 (1.0891)	Acc 0.750 (0.701)
Epoch: [9][19/61]	Time 0.21580 (0.33461)	Data 0.00020 (0.13224)	Loss 0.2604 (1.0455)	Acc 1.000 (0.717)
Epoch: [9][20/61]	Time 0.21597 (0.32868)	Data 0.00009 (0.12563)	Loss 0.7600 (1.0312)	Acc 0.625 (0.713)
Epoch: [9][21/61]	Time 0.21786 (0.32340)	Data 0.00009 (0.11965)	Loss 0.3976 (1.0011)	Acc 0.750 (0.714)
Epoch: [9][22/61]	Time 0.21688 (0.31856)	Data 0.00013 (0.11422)	Loss 1.0107 (1.0015)	Acc 0.750 (0.716)
Epoch: [9][23/61]	Time 0.21648 (0.31412)	Data 0.00010 (0.10926)	Loss 1.9612 (1.0432)	Acc 0.625 (0.712)
Epoch: [9][24/61]	Time 0.21611 (0.31004)	Data 0.00012 (0.10471)	Loss 0.8408 (1.0348)	Acc 0.625 (0.708)
Epoch: [9][25/61]	Time 0.21670 (0.30631)	Data 0.00009 (0.10053)	Loss 1.2397 (1.0430)	Acc 0.750 (0.710)
Epoch: [9][26/61]	Time 0.21703 (0.30287)	Data 0.00013 (0.09667)	Loss 0.4665 (1.0208)	Acc 0.875 (0.716)
Epoch: [9][27/61]	Time 0.21657 (0.29968)	Data 0.00013 (0.09309)	Loss 0.7551 (1.0110)	Acc 0.750 (0.718)
Epoch: [9][28/61]	Time 0.21659 (0.29671)	Data 0.00011 (0.08977)	Loss 1.1469 (1.0158)	Acc 0.750 (0.719)
Epoch: [9][29/61]	Time 0.21662 (0.29395)	Data 0.00009 (0.08668)	Loss 0.9927 (1.0150)	Acc 0.625 (0.716)
Epoch: [9][30/61]	Time 0.21608 (0.29135)	Data 0.00005 (0.08379)	Loss 0.5261 (0.9987)	Acc 0.875 (0.721)
Epoch: [9][31/61]	Time 0.21581 (0.28891)	Data 0.00005 (0.08109)	Loss 0.5077 (0.9829)	Acc 0.750 (0.722)
Epoch: [9][32/61]	Time 0.21595 (0.28663)	Data 0.00006 (0.07856)	Loss 0.7610 (0.9760)	Acc 0.750 (0.723)
Epoch: [9][33/61]	Time 0.21572 (0.28449)	Data 0.00008 (0.07618)	Loss 0.8456 (0.9720)	Acc 0.625 (0.720)
Epoch: [9][34/61]	Time 0.21601 (0.28247)	Data 0.00006 (0.07394)	Loss 0.7468 (0.9654)	Acc 0.750 (0.721)
Epoch: [9][35/61]	Time 0.21593 (0.28057)	Data 0.00008 (0.07183)	Loss 0.8006 (0.9607)	Acc 0.875 (0.725)
Epoch: [9][36/61]	Time 0.21542 (0.27876)	Data 0.00006 (0.06984)	Loss 1.8309 (0.9848)	Acc 0.375 (0.715)
Epoch: [9][37/61]	Time 0.21658 (0.27708)	Data 0.00004 (0.06795)	Loss 1.2652 (0.9924)	Acc 0.500 (0.709)
Epoch: [9][38/61]	Time 0.21516 (0.27545)	Data 0.00004 (0.06616)	Loss 0.3597 (0.9758)	Acc 1.000 (0.717)
Epoch: [9][39/61]	Time 0.21585 (0.27392)	Data 0.00004 (0.06447)	Loss 0.8309 (0.9721)	Acc 0.625 (0.715)
Epoch: [9][40/61]	Time 0.21521 (0.27245)	Data 0.00005 (0.06286)	Loss 1.5510 (0.9865)	Acc 0.500 (0.709)
Epoch: [9][41/61]	Time 0.21518 (0.27106)	Data 0.00004 (0.06132)	Loss 0.5280 (0.9754)	Acc 0.875 (0.713)
Epoch: [9][42/61]	Time 0.21586 (0.26974)	Data 0.00006 (0.05986)	Loss 0.4842 (0.9637)	Acc 0.750 (0.714)
Epoch: [9][43/61]	Time 0.21559 (0.26848)	Data 0.00005 (0.05847)	Loss 1.5293 (0.9768)	Acc 0.500 (0.709)
Epoch: [9][44/61]	Time 0.21544 (0.26728)	Data 0.00003 (0.05715)	Loss 0.4621 (0.9651)	Acc 0.875 (0.713)
Epoch: [9][45/61]	Time 0.21622 (0.26614)	Data 0.00005 (0.05588)	Loss 1.5843 (0.9789)	Acc 0.500 (0.708)
Epoch: [9][46/61]	Time 0.21567 (0.26505)	Data 0.00005 (0.05466)	Loss 0.6838 (0.9725)	Acc 0.875 (0.712)
Epoch: [9][47/61]	Time 0.21577 (0.26400)	Data 0.00005 (0.05350)	Loss 1.2319 (0.9780)	Acc 0.625 (0.710)
Epoch: [9][48/61]	Time 0.21569 (0.26299)	Data 0.00005 (0.05239)	Loss 1.5635 (0.9902)	Acc 0.625 (0.708)
Epoch: [9][49/61]	Time 0.21531 (0.26202)	Data 0.00005 (0.05132)	Loss 0.7889 (0.9861)	Acc 0.625 (0.707)
Epoch: [9][50/61]	Time 0.21574 (0.26109)	Data 0.00005 (0.05029)	Loss 1.5964 (0.9983)	Acc 0.500 (0.703)
Epoch: [9][51/61]	Time 0.21559 (0.26020)	Data 0.00008 (0.04931)	Loss 2.7046 (1.0317)	Acc 0.500 (0.699)
Epoch: [9][52/61]	Time 0.21619 (0.25935)	Data 0.00006 (0.04836)	Loss 0.5713 (1.0229)	Acc 0.625 (0.697)
Epoch: [9][53/61]	Time 0.21580 (0.25853)	Data 0.00005 (0.04745)	Loss 1.1296 (1.0249)	Acc 0.500 (0.693)
Epoch: [9][54/61]	Time 0.21581 (0.25774)	Data 0.00004 (0.04657)	Loss 0.4180 (1.0137)	Acc 0.625 (0.692)
Epoch: [9][55/61]	Time 0.21654 (0.25699)	Data 0.00007 (0.04573)	Loss 0.7902 (1.0096)	Acc 0.750 (0.693)
Epoch: [9][56/61]	Time 0.21546 (0.25625)	Data 0.00006 (0.04491)	Loss 0.9805 (1.0091)	Acc 0.500 (0.690)
Epoch: [9][57/61]	Time 0.21522 (0.25553)	Data 0.00005 (0.04412)	Loss 1.5419 (1.0184)	Acc 0.500 (0.686)
Epoch: [9][58/61]	Time 0.21529 (0.25484)	Data 0.00005 (0.04337)	Loss 0.9944 (1.0180)	Acc 0.750 (0.688)
Epoch: [9][59/61]	Time 0.21553 (0.25417)	Data 0.00005 (0.04263)	Loss 1.3737 (1.0240)	Acc 0.750 (0.689)
Epoch: [9][60/61]	Time 0.21609 (0.25354)	Data 0.00005 (0.04192)	Loss 1.5406 (1.0326)	Acc 0.625 (0.688)
Epoch: [9][61/61]	Time 0.07163 (0.25055)	Data 0.00003 (0.04123)	Loss 0.6327 (1.0310)	Acc 0.500 (0.687)
     Valid acc: 0.6867219917012448  (0.6597510373443983, ep: 8)
train at epoch 10
Epoch: [10][0/132]	 lr: 0.00100	Time 3.873 (3.873)	Data 3.043 (3.043)	Loss 1.2187 (1.2187)	Acc 0.625 (0.625)	Precision 0.444(0.444)	Recall 0.444(0.444)
Epoch: [10][10/132]	 lr: 0.00100	Time 0.812 (1.090)	Data 0.000 (0.277)	Loss 0.9464 (1.1472)	Acc 0.625 (0.648)	Precision 0.400(0.494)	Recall 0.400(0.479)
Epoch: [10][20/132]	 lr: 0.00100	Time 0.934 (1.001)	Data 0.000 (0.145)	Loss 1.0783 (1.2217)	Acc 0.750 (0.643)	Precision 0.625(0.497)	Recall 0.625(0.478)
Epoch: [10][30/132]	 lr: 0.00100	Time 0.932 (0.979)	Data 0.000 (0.098)	Loss 1.0365 (1.2173)	Acc 0.750 (0.653)	Precision 0.556(0.510)	Recall 0.556(0.488)
Epoch: [10][40/132]	 lr: 0.00100	Time 0.934 (0.968)	Data 0.000 (0.074)	Loss 1.5715 (1.1981)	Acc 0.500 (0.655)	Precision 0.300(0.514)	Recall 0.250(0.492)
Epoch: [10][50/132]	 lr: 0.00100	Time 0.935 (0.961)	Data 0.000 (0.060)	Loss 1.1701 (1.1762)	Acc 0.500 (0.650)	Precision 0.364(0.513)	Recall 0.364(0.490)
Epoch: [10][60/132]	 lr: 0.00100	Time 0.934 (0.957)	Data 0.000 (0.050)	Loss 1.0793 (1.1918)	Acc 0.500 (0.648)	Precision 0.364(0.521)	Recall 0.364(0.494)
Epoch: [10][70/132]	 lr: 0.00100	Time 0.935 (0.954)	Data 0.000 (0.043)	Loss 0.8857 (1.1821)	Acc 0.625 (0.646)	Precision 0.714(0.520)	Recall 0.548(0.494)
Epoch: [10][80/132]	 lr: 0.00100	Time 0.936 (0.951)	Data 0.000 (0.038)	Loss 0.8110 (1.1714)	Acc 0.750 (0.650)	Precision 0.667(0.524)	Recall 0.611(0.500)
Epoch: [10][90/132]	 lr: 0.00100	Time 0.932 (0.950)	Data 0.000 (0.034)	Loss 0.8767 (1.1892)	Acc 0.750 (0.643)	Precision 0.625(0.521)	Recall 0.562(0.494)
Epoch: [10][100/132]	 lr: 0.00100	Time 0.933 (0.948)	Data 0.000 (0.030)	Loss 0.8529 (1.1762)	Acc 0.750 (0.642)	Precision 0.625(0.518)	Recall 0.562(0.492)
Epoch: [10][110/132]	 lr: 0.00100	Time 0.935 (0.947)	Data 0.000 (0.028)	Loss 1.7386 (1.1607)	Acc 0.500 (0.648)	Precision 0.333(0.526)	Recall 0.222(0.498)
Epoch: [10][120/132]	 lr: 0.00100	Time 0.935 (0.946)	Data 0.000 (0.025)	Loss 0.9187 (1.1431)	Acc 0.625 (0.651)	Precision 0.400(0.528)	Recall 0.400(0.500)
Epoch: [10][130/132]	 lr: 0.00100	Time 0.936 (0.945)	Data 0.000 (0.023)	Loss 1.1389 (1.1226)	Acc 0.500 (0.658)	Precision 0.389(0.538)	Recall 0.389(0.511)
validation at epoch 10
Epoch: [10][1/61]	Time 2.75019 (2.75019)	Data 2.54195 (2.54195)	Loss 1.2834 (1.2834)	Acc 0.500 (0.500)
Epoch: [10][2/61]	Time 0.19289 (1.47154)	Data 0.00010 (1.27102)	Loss 0.3207 (0.8020)	Acc 0.875 (0.688)
Epoch: [10][3/61]	Time 0.19022 (1.04443)	Data 0.00014 (0.84740)	Loss 0.8944 (0.8328)	Acc 0.625 (0.667)
Epoch: [10][4/61]	Time 0.19075 (0.83101)	Data 0.00011 (0.63558)	Loss 1.4688 (0.9918)	Acc 0.500 (0.625)
Epoch: [10][5/61]	Time 0.19297 (0.70340)	Data 0.00012 (0.50848)	Loss 0.4421 (0.8819)	Acc 0.875 (0.675)
Epoch: [10][6/61]	Time 0.19440 (0.61857)	Data 0.00009 (0.42375)	Loss 0.1119 (0.7535)	Acc 1.000 (0.729)
Epoch: [10][7/61]	Time 0.19583 (0.55818)	Data 0.00012 (0.36323)	Loss 0.2879 (0.6870)	Acc 1.000 (0.768)
Epoch: [10][8/61]	Time 0.19841 (0.51321)	Data 0.00010 (0.31784)	Loss 0.6516 (0.6826)	Acc 0.875 (0.781)
Epoch: [10][9/61]	Time 0.19967 (0.47837)	Data 0.00011 (0.28254)	Loss 0.3252 (0.6429)	Acc 0.875 (0.792)
Epoch: [10][10/61]	Time 0.20061 (0.45059)	Data 0.00012 (0.25430)	Loss 1.2758 (0.7062)	Acc 0.750 (0.787)
Epoch: [10][11/61]	Time 0.20312 (0.42810)	Data 0.00016 (0.23119)	Loss 1.7721 (0.8031)	Acc 0.500 (0.761)
Epoch: [10][12/61]	Time 0.20516 (0.40952)	Data 0.00012 (0.21194)	Loss 0.3618 (0.7663)	Acc 0.875 (0.771)
Epoch: [10][13/61]	Time 0.20698 (0.39394)	Data 0.00011 (0.19564)	Loss 0.7303 (0.7635)	Acc 0.750 (0.769)
Epoch: [10][14/61]	Time 0.20923 (0.38075)	Data 0.00009 (0.18167)	Loss 1.2007 (0.7948)	Acc 0.625 (0.759)
Epoch: [10][15/61]	Time 0.21049 (0.36940)	Data 0.00012 (0.16957)	Loss 1.2123 (0.8226)	Acc 0.750 (0.758)
Epoch: [10][16/61]	Time 0.21285 (0.35961)	Data 0.00014 (0.15898)	Loss 1.2711 (0.8506)	Acc 0.500 (0.742)
Epoch: [10][17/61]	Time 0.21555 (0.35114)	Data 0.00014 (0.14964)	Loss 0.4973 (0.8298)	Acc 0.875 (0.750)
Epoch: [10][18/61]	Time 0.21639 (0.34365)	Data 0.00014 (0.14133)	Loss 0.2787 (0.7992)	Acc 0.875 (0.757)
Epoch: [10][19/61]	Time 0.21618 (0.33694)	Data 0.00012 (0.13390)	Loss 0.4763 (0.7822)	Acc 0.750 (0.757)
Epoch: [10][20/61]	Time 0.21592 (0.33089)	Data 0.00009 (0.12721)	Loss 0.3440 (0.7603)	Acc 0.875 (0.762)
Epoch: [10][21/61]	Time 0.21604 (0.32542)	Data 0.00009 (0.12116)	Loss 0.3874 (0.7426)	Acc 0.875 (0.768)
Epoch: [10][22/61]	Time 0.21665 (0.32048)	Data 0.00010 (0.11565)	Loss 0.7189 (0.7415)	Acc 0.625 (0.761)
Epoch: [10][23/61]	Time 0.21576 (0.31592)	Data 0.00012 (0.11063)	Loss 1.3378 (0.7674)	Acc 0.750 (0.761)
Epoch: [10][24/61]	Time 0.21584 (0.31175)	Data 0.00010 (0.10603)	Loss 0.3807 (0.7513)	Acc 1.000 (0.771)
Epoch: [10][25/61]	Time 0.21627 (0.30794)	Data 0.00009 (0.10179)	Loss 0.2322 (0.7305)	Acc 1.000 (0.780)
Epoch: [10][26/61]	Time 0.21635 (0.30441)	Data 0.00012 (0.09788)	Loss 0.2543 (0.7122)	Acc 0.875 (0.784)
Epoch: [10][27/61]	Time 0.21564 (0.30112)	Data 0.00012 (0.09426)	Loss 0.7459 (0.7135)	Acc 0.625 (0.778)
Epoch: [10][28/61]	Time 0.21608 (0.29809)	Data 0.00009 (0.09089)	Loss 0.5146 (0.7064)	Acc 0.875 (0.781)
Epoch: [10][29/61]	Time 0.21608 (0.29526)	Data 0.00010 (0.08776)	Loss 0.6884 (0.7057)	Acc 0.875 (0.784)
Epoch: [10][30/61]	Time 0.21648 (0.29263)	Data 0.00006 (0.08484)	Loss 0.9111 (0.7126)	Acc 0.750 (0.783)
Epoch: [10][31/61]	Time 0.21605 (0.29016)	Data 0.00005 (0.08210)	Loss 0.4037 (0.7026)	Acc 0.875 (0.786)
Epoch: [10][32/61]	Time 0.21582 (0.28784)	Data 0.00009 (0.07954)	Loss 0.5390 (0.6975)	Acc 0.750 (0.785)
Epoch: [10][33/61]	Time 0.21597 (0.28566)	Data 0.00009 (0.07713)	Loss 0.8957 (0.7035)	Acc 0.625 (0.780)
Epoch: [10][34/61]	Time 0.21540 (0.28360)	Data 0.00009 (0.07487)	Loss 0.5286 (0.6984)	Acc 0.875 (0.783)
Epoch: [10][35/61]	Time 0.21567 (0.28165)	Data 0.00004 (0.07273)	Loss 0.5945 (0.6954)	Acc 0.875 (0.786)
Epoch: [10][36/61]	Time 0.21580 (0.27983)	Data 0.00003 (0.07071)	Loss 0.9609 (0.7028)	Acc 0.625 (0.781)
Epoch: [10][37/61]	Time 0.21610 (0.27810)	Data 0.00005 (0.06880)	Loss 0.3308 (0.6927)	Acc 1.000 (0.787)
Epoch: [10][38/61]	Time 0.21602 (0.27647)	Data 0.00004 (0.06699)	Loss 0.4845 (0.6872)	Acc 0.875 (0.789)
Epoch: [10][39/61]	Time 0.21534 (0.27490)	Data 0.00007 (0.06528)	Loss 0.5059 (0.6826)	Acc 0.750 (0.788)
Epoch: [10][40/61]	Time 0.21578 (0.27342)	Data 0.00006 (0.06364)	Loss 1.0322 (0.6913)	Acc 0.625 (0.784)
Epoch: [10][41/61]	Time 0.21571 (0.27202)	Data 0.00004 (0.06209)	Loss 0.3750 (0.6836)	Acc 0.875 (0.787)
Epoch: [10][42/61]	Time 0.21582 (0.27068)	Data 0.00005 (0.06062)	Loss 0.3425 (0.6755)	Acc 0.875 (0.789)
Epoch: [10][43/61]	Time 0.21562 (0.26940)	Data 0.00005 (0.05921)	Loss 1.1216 (0.6859)	Acc 0.625 (0.785)
Epoch: [10][44/61]	Time 0.21575 (0.26818)	Data 0.00005 (0.05786)	Loss 0.4239 (0.6799)	Acc 0.750 (0.784)
Epoch: [10][45/61]	Time 0.21544 (0.26701)	Data 0.00006 (0.05658)	Loss 1.5508 (0.6993)	Acc 0.625 (0.781)
Epoch: [10][46/61]	Time 0.21559 (0.26589)	Data 0.00006 (0.05535)	Loss 0.1344 (0.6870)	Acc 1.000 (0.785)
Epoch: [10][47/61]	Time 0.21501 (0.26481)	Data 0.00005 (0.05417)	Loss 0.6067 (0.6853)	Acc 0.750 (0.785)
Epoch: [10][48/61]	Time 0.21542 (0.26378)	Data 0.00005 (0.05305)	Loss 0.5638 (0.6828)	Acc 0.750 (0.784)
Epoch: [10][49/61]	Time 0.21587 (0.26280)	Data 0.00006 (0.05196)	Loss 1.0554 (0.6904)	Acc 0.625 (0.781)
Epoch: [10][50/61]	Time 0.21542 (0.26185)	Data 0.00003 (0.05093)	Loss 0.4636 (0.6858)	Acc 0.875 (0.782)
Epoch: [10][51/61]	Time 0.21545 (0.26094)	Data 0.00005 (0.04993)	Loss 1.8644 (0.7089)	Acc 0.500 (0.777)
Epoch: [10][52/61]	Time 0.21547 (0.26007)	Data 0.00004 (0.04897)	Loss 0.4220 (0.7034)	Acc 0.750 (0.776)
Epoch: [10][53/61]	Time 0.21586 (0.25923)	Data 0.00004 (0.04805)	Loss 0.4564 (0.6988)	Acc 0.750 (0.776)
Epoch: [10][54/61]	Time 0.21527 (0.25842)	Data 0.00006 (0.04716)	Loss 0.3096 (0.6915)	Acc 0.875 (0.778)
Epoch: [10][55/61]	Time 0.21578 (0.25764)	Data 0.00006 (0.04630)	Loss 0.3050 (0.6845)	Acc 0.875 (0.780)
Epoch: [10][56/61]	Time 0.21560 (0.25689)	Data 0.00005 (0.04547)	Loss 0.7414 (0.6855)	Acc 0.750 (0.779)
Epoch: [10][57/61]	Time 0.21560 (0.25617)	Data 0.00005 (0.04468)	Loss 0.9720 (0.6906)	Acc 0.750 (0.779)
Epoch: [10][58/61]	Time 0.21563 (0.25547)	Data 0.00005 (0.04391)	Loss 0.2934 (0.6837)	Acc 0.875 (0.780)
Epoch: [10][59/61]	Time 0.21555 (0.25479)	Data 0.00005 (0.04317)	Loss 1.4411 (0.6965)	Acc 0.750 (0.780)
Epoch: [10][60/61]	Time 0.21549 (0.25414)	Data 0.00005 (0.04245)	Loss 1.3143 (0.7068)	Acc 0.750 (0.779)
Epoch: [10][61/61]	Time 0.07155 (0.25114)	Data 0.00002 (0.04175)	Loss 0.2289 (0.7049)	Acc 1.000 (0.780)
     Valid acc: 0.7800829875518672  (0.6867219917012448, ep: 9)
train at epoch 11
Epoch: [11][0/132]	 lr: 0.00100	Time 3.783 (3.783)	Data 2.950 (2.950)	Loss 0.7658 (0.7658)	Acc 0.875 (0.875)	Precision 0.714(0.714)	Recall 0.714(0.714)
Epoch: [11][10/132]	 lr: 0.00100	Time 0.812 (1.081)	Data 0.000 (0.268)	Loss 0.7401 (1.0436)	Acc 0.875 (0.716)	Precision 0.875(0.611)	Recall 0.812(0.591)
Epoch: [11][20/132]	 lr: 0.00100	Time 0.934 (1.000)	Data 0.000 (0.141)	Loss 0.8449 (1.0697)	Acc 0.750 (0.702)	Precision 0.688(0.608)	Recall 0.688(0.581)
Epoch: [11][30/132]	 lr: 0.00100	Time 0.933 (0.978)	Data 0.000 (0.095)	Loss 0.6774 (1.0173)	Acc 0.875 (0.706)	Precision 0.778(0.609)	Recall 0.778(0.582)
Epoch: [11][40/132]	 lr: 0.00100	Time 0.933 (0.968)	Data 0.000 (0.072)	Loss 0.3646 (1.0219)	Acc 1.000 (0.710)	Precision 1.000(0.612)	Recall 1.000(0.583)
Epoch: [11][50/132]	 lr: 0.00100	Time 0.934 (0.961)	Data 0.000 (0.058)	Loss 1.1689 (1.0120)	Acc 0.500 (0.708)	Precision 0.364(0.611)	Recall 0.364(0.581)
Epoch: [11][60/132]	 lr: 0.00100	Time 0.935 (0.957)	Data 0.000 (0.048)	Loss 0.8457 (1.0283)	Acc 0.625 (0.699)	Precision 0.500(0.603)	Recall 0.450(0.569)
Epoch: [11][70/132]	 lr: 0.00100	Time 0.934 (0.953)	Data 0.000 (0.042)	Loss 0.9758 (1.0060)	Acc 0.625 (0.697)	Precision 0.500(0.598)	Recall 0.450(0.568)
Epoch: [11][80/132]	 lr: 0.00100	Time 0.933 (0.951)	Data 0.000 (0.037)	Loss 0.8414 (1.0082)	Acc 0.750 (0.696)	Precision 0.556(0.599)	Recall 0.556(0.568)
Epoch: [11][90/132]	 lr: 0.00100	Time 0.932 (0.949)	Data 0.000 (0.033)	Loss 0.9619 (0.9884)	Acc 0.875 (0.706)	Precision 0.875(0.610)	Recall 0.812(0.580)
Epoch: [11][100/132]	 lr: 0.00100	Time 0.932 (0.947)	Data 0.000 (0.029)	Loss 0.5778 (0.9767)	Acc 0.750 (0.704)	Precision 0.667(0.606)	Recall 0.611(0.575)
Epoch: [11][110/132]	 lr: 0.00100	Time 0.932 (0.946)	Data 0.000 (0.027)	Loss 0.8204 (0.9493)	Acc 0.875 (0.715)	Precision 0.875(0.620)	Recall 0.812(0.588)
Epoch: [11][120/132]	 lr: 0.00100	Time 0.934 (0.945)	Data 0.000 (0.025)	Loss 0.8286 (0.9333)	Acc 0.750 (0.720)	Precision 0.556(0.625)	Recall 0.556(0.595)
Epoch: [11][130/132]	 lr: 0.00100	Time 0.933 (0.944)	Data 0.000 (0.023)	Loss 0.4338 (0.9435)	Acc 0.875 (0.720)	Precision 0.875(0.626)	Recall 0.812(0.596)
validation at epoch 11
Epoch: [11][1/61]	Time 2.74234 (2.74234)	Data 2.52976 (2.52976)	Loss 1.3316 (1.3316)	Acc 0.500 (0.500)
Epoch: [11][2/61]	Time 0.19511 (1.46873)	Data 0.00017 (1.26496)	Loss 0.3099 (0.8207)	Acc 0.875 (0.688)
Epoch: [11][3/61]	Time 0.18989 (1.04245)	Data 0.00009 (0.84334)	Loss 0.8267 (0.8227)	Acc 0.750 (0.708)
Epoch: [11][4/61]	Time 0.19060 (0.82949)	Data 0.00010 (0.63253)	Loss 1.3012 (0.9423)	Acc 0.625 (0.688)
Epoch: [11][5/61]	Time 0.19129 (0.70185)	Data 0.00016 (0.50606)	Loss 0.3785 (0.8296)	Acc 0.875 (0.725)
Epoch: [11][6/61]	Time 0.19282 (0.61701)	Data 0.00009 (0.42173)	Loss 0.1006 (0.7081)	Acc 1.000 (0.771)
Epoch: [11][7/61]	Time 0.19434 (0.55663)	Data 0.00009 (0.36149)	Loss 0.2744 (0.6461)	Acc 0.875 (0.786)
Epoch: [11][8/61]	Time 0.19663 (0.51163)	Data 0.00009 (0.31632)	Loss 0.6754 (0.6498)	Acc 0.875 (0.797)
Epoch: [11][9/61]	Time 0.19746 (0.47672)	Data 0.00016 (0.28119)	Loss 0.3777 (0.6195)	Acc 0.875 (0.806)
Epoch: [11][10/61]	Time 0.20028 (0.44908)	Data 0.00009 (0.25308)	Loss 1.1118 (0.6688)	Acc 0.750 (0.800)
Epoch: [11][11/61]	Time 0.20156 (0.42658)	Data 0.00014 (0.23009)	Loss 1.4770 (0.7422)	Acc 0.375 (0.761)
Epoch: [11][12/61]	Time 0.20309 (0.40795)	Data 0.00009 (0.21092)	Loss 0.3284 (0.7078)	Acc 0.875 (0.771)
Epoch: [11][13/61]	Time 0.20555 (0.39238)	Data 0.00008 (0.19470)	Loss 0.6397 (0.7025)	Acc 0.750 (0.769)
Epoch: [11][14/61]	Time 0.20726 (0.37916)	Data 0.00009 (0.18080)	Loss 0.9630 (0.7211)	Acc 0.750 (0.768)
Epoch: [11][15/61]	Time 0.20909 (0.36782)	Data 0.00009 (0.16875)	Loss 1.2919 (0.7592)	Acc 0.750 (0.767)
Epoch: [11][16/61]	Time 0.21140 (0.35805)	Data 0.00009 (0.15821)	Loss 0.9481 (0.7710)	Acc 0.500 (0.750)
Epoch: [11][17/61]	Time 0.21204 (0.34946)	Data 0.00014 (0.14891)	Loss 0.3744 (0.7477)	Acc 0.875 (0.757)
Epoch: [11][18/61]	Time 0.21569 (0.34203)	Data 0.00011 (0.14065)	Loss 0.1781 (0.7160)	Acc 1.000 (0.771)
Epoch: [11][19/61]	Time 0.21667 (0.33543)	Data 0.00015 (0.13325)	Loss 0.4953 (0.7044)	Acc 0.750 (0.770)
Epoch: [11][20/61]	Time 0.21693 (0.32950)	Data 0.00012 (0.12660)	Loss 0.3663 (0.6875)	Acc 0.875 (0.775)
Epoch: [11][21/61]	Time 0.21618 (0.32411)	Data 0.00014 (0.12057)	Loss 0.3172 (0.6699)	Acc 0.875 (0.780)
Epoch: [11][22/61]	Time 0.21713 (0.31924)	Data 0.00009 (0.11510)	Loss 0.4813 (0.6613)	Acc 1.000 (0.790)
Epoch: [11][23/61]	Time 0.21623 (0.31477)	Data 0.00009 (0.11010)	Loss 1.3103 (0.6895)	Acc 0.750 (0.788)
Epoch: [11][24/61]	Time 0.21677 (0.31068)	Data 0.00012 (0.10551)	Loss 0.4870 (0.6811)	Acc 0.875 (0.792)
Epoch: [11][25/61]	Time 0.21591 (0.30689)	Data 0.00009 (0.10130)	Loss 0.2196 (0.6626)	Acc 1.000 (0.800)
Epoch: [11][26/61]	Time 0.21692 (0.30343)	Data 0.00009 (0.09740)	Loss 0.2090 (0.6452)	Acc 1.000 (0.808)
Epoch: [11][27/61]	Time 0.21537 (0.30017)	Data 0.00012 (0.09380)	Loss 0.6589 (0.6457)	Acc 0.750 (0.806)
Epoch: [11][28/61]	Time 0.21674 (0.29719)	Data 0.00008 (0.09045)	Loss 0.5134 (0.6409)	Acc 0.875 (0.808)
Epoch: [11][29/61]	Time 0.21618 (0.29440)	Data 0.00011 (0.08734)	Loss 0.7208 (0.6437)	Acc 0.750 (0.806)
Epoch: [11][30/61]	Time 0.21567 (0.29177)	Data 0.00006 (0.08443)	Loss 0.7937 (0.6487)	Acc 0.750 (0.804)
Epoch: [11][31/61]	Time 0.21614 (0.28933)	Data 0.00006 (0.08171)	Loss 0.3963 (0.6406)	Acc 0.875 (0.806)
Epoch: [11][32/61]	Time 0.21659 (0.28706)	Data 0.00005 (0.07916)	Loss 0.5819 (0.6387)	Acc 0.750 (0.805)
Epoch: [11][33/61]	Time 0.21568 (0.28490)	Data 0.00010 (0.07676)	Loss 0.6655 (0.6395)	Acc 0.875 (0.807)
Epoch: [11][34/61]	Time 0.21531 (0.28285)	Data 0.00005 (0.07450)	Loss 0.4552 (0.6341)	Acc 0.875 (0.809)
Epoch: [11][35/61]	Time 0.21539 (0.28092)	Data 0.00007 (0.07238)	Loss 0.7023 (0.6361)	Acc 0.875 (0.811)
Epoch: [11][36/61]	Time 0.21586 (0.27911)	Data 0.00005 (0.07037)	Loss 0.9966 (0.6461)	Acc 0.625 (0.806)
Epoch: [11][37/61]	Time 0.21583 (0.27740)	Data 0.00007 (0.06847)	Loss 0.2963 (0.6366)	Acc 1.000 (0.811)
Epoch: [11][38/61]	Time 0.21545 (0.27577)	Data 0.00006 (0.06667)	Loss 0.4247 (0.6310)	Acc 0.875 (0.812)
Epoch: [11][39/61]	Time 0.21576 (0.27424)	Data 0.00005 (0.06496)	Loss 0.4574 (0.6266)	Acc 0.750 (0.811)
Epoch: [11][40/61]	Time 0.21567 (0.27277)	Data 0.00007 (0.06334)	Loss 1.1352 (0.6393)	Acc 0.750 (0.809)
Epoch: [11][41/61]	Time 0.21551 (0.27137)	Data 0.00004 (0.06179)	Loss 0.3400 (0.6320)	Acc 0.875 (0.811)
Epoch: [11][42/61]	Time 0.21544 (0.27004)	Data 0.00003 (0.06032)	Loss 0.3208 (0.6246)	Acc 0.875 (0.812)
Epoch: [11][43/61]	Time 0.21558 (0.26878)	Data 0.00006 (0.05892)	Loss 0.9858 (0.6330)	Acc 0.625 (0.808)
Epoch: [11][44/61]	Time 0.21565 (0.26757)	Data 0.00003 (0.05758)	Loss 0.5002 (0.6300)	Acc 0.750 (0.807)
Epoch: [11][45/61]	Time 0.21596 (0.26642)	Data 0.00006 (0.05631)	Loss 1.5147 (0.6496)	Acc 0.625 (0.803)
Epoch: [11][46/61]	Time 0.21599 (0.26533)	Data 0.00005 (0.05508)	Loss 0.1562 (0.6389)	Acc 0.875 (0.804)
Epoch: [11][47/61]	Time 0.21578 (0.26427)	Data 0.00005 (0.05391)	Loss 0.5071 (0.6361)	Acc 0.875 (0.806)
Epoch: [11][48/61]	Time 0.21574 (0.26326)	Data 0.00005 (0.05279)	Loss 0.6492 (0.6364)	Acc 0.750 (0.805)
Epoch: [11][49/61]	Time 0.21572 (0.26229)	Data 0.00007 (0.05171)	Loss 1.2986 (0.6499)	Acc 0.750 (0.804)
Epoch: [11][50/61]	Time 0.21560 (0.26136)	Data 0.00005 (0.05068)	Loss 0.4146 (0.6452)	Acc 0.875 (0.805)
Epoch: [11][51/61]	Time 0.21583 (0.26046)	Data 0.00004 (0.04969)	Loss 2.0391 (0.6725)	Acc 0.500 (0.799)
Epoch: [11][52/61]	Time 0.21594 (0.25961)	Data 0.00004 (0.04873)	Loss 0.4053 (0.6674)	Acc 0.750 (0.798)
Epoch: [11][53/61]	Time 0.21597 (0.25878)	Data 0.00004 (0.04781)	Loss 0.5145 (0.6645)	Acc 0.750 (0.797)
Epoch: [11][54/61]	Time 0.21547 (0.25798)	Data 0.00006 (0.04693)	Loss 0.3175 (0.6581)	Acc 0.875 (0.799)
Epoch: [11][55/61]	Time 0.21567 (0.25721)	Data 0.00005 (0.04608)	Loss 0.3115 (0.6518)	Acc 0.875 (0.800)
Epoch: [11][56/61]	Time 0.21588 (0.25647)	Data 0.00005 (0.04526)	Loss 0.7060 (0.6527)	Acc 0.875 (0.801)
Epoch: [11][57/61]	Time 0.21536 (0.25575)	Data 0.00006 (0.04446)	Loss 0.7916 (0.6552)	Acc 0.750 (0.800)
Epoch: [11][58/61]	Time 0.21548 (0.25506)	Data 0.00005 (0.04370)	Loss 0.2926 (0.6489)	Acc 0.875 (0.802)
Epoch: [11][59/61]	Time 0.21527 (0.25438)	Data 0.00005 (0.04296)	Loss 1.3961 (0.6616)	Acc 0.750 (0.801)
Epoch: [11][60/61]	Time 0.21579 (0.25374)	Data 0.00005 (0.04224)	Loss 1.0777 (0.6685)	Acc 0.750 (0.800)
Epoch: [11][61/61]	Time 0.07172 (0.25076)	Data 0.00004 (0.04155)	Loss 0.2319 (0.6667)	Acc 1.000 (0.801)
     Valid acc: 0.8008298755186722  (0.7800829875518672, ep: 10)
train at epoch 12
Epoch: [12][0/132]	 lr: 0.00100	Time 3.749 (3.749)	Data 2.921 (2.921)	Loss 1.2197 (1.2197)	Acc 0.750 (0.750)	Precision 0.667(0.667)	Recall 0.611(0.611)
Epoch: [12][10/132]	 lr: 0.00100	Time 0.810 (1.077)	Data 0.000 (0.266)	Loss 0.8499 (0.9555)	Acc 0.625 (0.761)	Precision 0.455(0.667)	Recall 0.455(0.642)
Epoch: [12][20/132]	 lr: 0.00100	Time 0.932 (0.997)	Data 0.000 (0.139)	Loss 0.2481 (0.9679)	Acc 1.000 (0.750)	Precision 1.000(0.659)	Recall 1.000(0.624)
Epoch: [12][30/132]	 lr: 0.00100	Time 0.932 (0.976)	Data 0.000 (0.094)	Loss 0.9025 (0.9920)	Acc 0.750 (0.718)	Precision 0.667(0.616)	Recall 0.667(0.592)
Epoch: [12][40/132]	 lr: 0.00100	Time 0.933 (0.965)	Data 0.000 (0.071)	Loss 1.4186 (1.0027)	Acc 0.625 (0.723)	Precision 0.556(0.625)	Recall 0.500(0.603)
Epoch: [12][50/132]	 lr: 0.00100	Time 0.929 (0.959)	Data 0.000 (0.057)	Loss 0.7337 (0.9662)	Acc 0.750 (0.733)	Precision 0.556(0.634)	Recall 0.556(0.615)
Epoch: [12][60/132]	 lr: 0.00100	Time 0.932 (0.955)	Data 0.000 (0.048)	Loss 0.8937 (0.9707)	Acc 0.750 (0.727)	Precision 0.625(0.624)	Recall 0.562(0.607)
Epoch: [12][70/132]	 lr: 0.00100	Time 0.932 (0.951)	Data 0.000 (0.041)	Loss 0.4402 (0.9520)	Acc 1.000 (0.739)	Precision 1.000(0.643)	Recall 1.000(0.625)
Epoch: [12][80/132]	 lr: 0.00100	Time 0.931 (0.949)	Data 0.000 (0.036)	Loss 1.1309 (0.9450)	Acc 0.750 (0.745)	Precision 0.750(0.647)	Recall 0.625(0.628)
Epoch: [12][90/132]	 lr: 0.00100	Time 0.930 (0.947)	Data 0.000 (0.032)	Loss 0.5608 (0.9391)	Acc 0.875 (0.743)	Precision 0.857(0.646)	Recall 0.810(0.626)
Epoch: [12][100/132]	 lr: 0.00100	Time 0.930 (0.945)	Data 0.000 (0.029)	Loss 0.5595 (0.9329)	Acc 0.875 (0.741)	Precision 0.778(0.645)	Recall 0.778(0.624)
Epoch: [12][110/132]	 lr: 0.00100	Time 0.931 (0.944)	Data 0.000 (0.026)	Loss 0.8469 (0.9214)	Acc 0.625 (0.741)	Precision 0.400(0.642)	Recall 0.400(0.621)
Epoch: [12][120/132]	 lr: 0.00100	Time 0.932 (0.943)	Data 0.000 (0.024)	Loss 0.4906 (0.9258)	Acc 0.875 (0.738)	Precision 0.778(0.639)	Recall 0.778(0.615)
Epoch: [12][130/132]	 lr: 0.00100	Time 0.932 (0.942)	Data 0.000 (0.022)	Loss 1.2071 (0.9312)	Acc 0.625 (0.733)	Precision 0.455(0.632)	Recall 0.455(0.610)
validation at epoch 12
Epoch: [12][1/61]	Time 2.62110 (2.62110)	Data 2.41178 (2.41178)	Loss 1.1828 (1.1828)	Acc 0.625 (0.625)
Epoch: [12][2/61]	Time 0.19360 (1.40735)	Data 0.00017 (1.20597)	Loss 0.2706 (0.7267)	Acc 0.875 (0.750)
Epoch: [12][3/61]	Time 0.19008 (1.00159)	Data 0.00008 (0.80401)	Loss 0.7914 (0.7483)	Acc 0.750 (0.750)
Epoch: [12][4/61]	Time 0.19105 (0.79895)	Data 0.00012 (0.60304)	Loss 1.0729 (0.8294)	Acc 0.625 (0.719)
Epoch: [12][5/61]	Time 0.19295 (0.67775)	Data 0.00009 (0.48245)	Loss 0.5024 (0.7640)	Acc 0.875 (0.750)
Epoch: [12][6/61]	Time 0.19477 (0.59726)	Data 0.00009 (0.40205)	Loss 0.1346 (0.6591)	Acc 1.000 (0.792)
Epoch: [12][7/61]	Time 0.19604 (0.53994)	Data 0.00009 (0.34463)	Loss 0.2440 (0.5998)	Acc 1.000 (0.821)
Epoch: [12][8/61]	Time 0.19748 (0.49713)	Data 0.00009 (0.30156)	Loss 0.8239 (0.6278)	Acc 0.750 (0.812)
Epoch: [12][9/61]	Time 0.19954 (0.46407)	Data 0.00009 (0.26807)	Loss 0.1742 (0.5774)	Acc 1.000 (0.833)
Epoch: [12][10/61]	Time 0.20084 (0.43774)	Data 0.00009 (0.24127)	Loss 1.1980 (0.6395)	Acc 0.750 (0.825)
Epoch: [12][11/61]	Time 0.20330 (0.41643)	Data 0.00009 (0.21934)	Loss 1.3531 (0.7044)	Acc 0.375 (0.784)
Epoch: [12][12/61]	Time 0.20585 (0.39888)	Data 0.00009 (0.20107)	Loss 0.2158 (0.6636)	Acc 0.875 (0.792)
Epoch: [12][13/61]	Time 0.20730 (0.38414)	Data 0.00009 (0.18561)	Loss 0.5893 (0.6579)	Acc 0.750 (0.788)
Epoch: [12][14/61]	Time 0.20957 (0.37168)	Data 0.00011 (0.17236)	Loss 1.2276 (0.6986)	Acc 0.625 (0.777)
Epoch: [12][15/61]	Time 0.21167 (0.36101)	Data 0.00011 (0.16088)	Loss 1.4250 (0.7470)	Acc 0.750 (0.775)
Epoch: [12][16/61]	Time 0.21212 (0.35170)	Data 0.00009 (0.15083)	Loss 0.6962 (0.7439)	Acc 0.750 (0.773)
Epoch: [12][17/61]	Time 0.21594 (0.34372)	Data 0.00018 (0.14197)	Loss 0.4375 (0.7258)	Acc 0.875 (0.779)
Epoch: [12][18/61]	Time 0.21580 (0.33661)	Data 0.00014 (0.13409)	Loss 0.1902 (0.6961)	Acc 0.875 (0.785)
Epoch: [12][19/61]	Time 0.21662 (0.33029)	Data 0.00011 (0.12704)	Loss 0.3983 (0.6804)	Acc 0.875 (0.789)
Epoch: [12][20/61]	Time 0.21593 (0.32458)	Data 0.00012 (0.12069)	Loss 0.4706 (0.6699)	Acc 0.750 (0.787)
Epoch: [12][21/61]	Time 0.21587 (0.31940)	Data 0.00009 (0.11495)	Loss 0.3656 (0.6554)	Acc 0.875 (0.792)
Epoch: [12][22/61]	Time 0.21603 (0.31470)	Data 0.00013 (0.10973)	Loss 0.5240 (0.6495)	Acc 0.875 (0.795)
Epoch: [12][23/61]	Time 0.21580 (0.31040)	Data 0.00008 (0.10496)	Loss 1.2683 (0.6764)	Acc 0.750 (0.793)
Epoch: [12][24/61]	Time 0.21624 (0.30648)	Data 0.00008 (0.10059)	Loss 0.5477 (0.6710)	Acc 0.750 (0.792)
Epoch: [12][25/61]	Time 0.21618 (0.30287)	Data 0.00009 (0.09657)	Loss 0.2275 (0.6533)	Acc 1.000 (0.800)
Epoch: [12][26/61]	Time 0.21629 (0.29954)	Data 0.00011 (0.09286)	Loss 0.2196 (0.6366)	Acc 0.875 (0.803)
Epoch: [12][27/61]	Time 0.21619 (0.29645)	Data 0.00012 (0.08943)	Loss 0.5225 (0.6324)	Acc 0.750 (0.801)
Epoch: [12][28/61]	Time 0.21602 (0.29358)	Data 0.00011 (0.08624)	Loss 0.5826 (0.6306)	Acc 0.875 (0.804)
Epoch: [12][29/61]	Time 0.21603 (0.29090)	Data 0.00009 (0.08327)	Loss 0.6437 (0.6310)	Acc 0.875 (0.806)
Epoch: [12][30/61]	Time 0.21575 (0.28840)	Data 0.00006 (0.08049)	Loss 0.8543 (0.6385)	Acc 0.750 (0.804)
Epoch: [12][31/61]	Time 0.21568 (0.28605)	Data 0.00006 (0.07790)	Loss 0.3831 (0.6302)	Acc 0.875 (0.806)
Epoch: [12][32/61]	Time 0.21565 (0.28385)	Data 0.00006 (0.07547)	Loss 0.5495 (0.6277)	Acc 0.750 (0.805)
Epoch: [12][33/61]	Time 0.21543 (0.28178)	Data 0.00010 (0.07318)	Loss 0.5188 (0.6244)	Acc 1.000 (0.811)
Epoch: [12][34/61]	Time 0.21516 (0.27982)	Data 0.00007 (0.07103)	Loss 0.3906 (0.6175)	Acc 0.875 (0.812)
Epoch: [12][35/61]	Time 0.21525 (0.27797)	Data 0.00004 (0.06900)	Loss 0.7007 (0.6199)	Acc 0.875 (0.814)
Epoch: [12][36/61]	Time 0.21595 (0.27625)	Data 0.00004 (0.06709)	Loss 0.9251 (0.6284)	Acc 0.625 (0.809)
Epoch: [12][37/61]	Time 0.21511 (0.27460)	Data 0.00004 (0.06527)	Loss 0.2363 (0.6178)	Acc 1.000 (0.814)
Epoch: [12][38/61]	Time 0.21538 (0.27304)	Data 0.00006 (0.06356)	Loss 0.4504 (0.6134)	Acc 0.875 (0.816)
Epoch: [12][39/61]	Time 0.21536 (0.27156)	Data 0.00004 (0.06193)	Loss 0.4969 (0.6104)	Acc 0.750 (0.814)
Epoch: [12][40/61]	Time 0.21564 (0.27016)	Data 0.00004 (0.06038)	Loss 1.1606 (0.6241)	Acc 0.875 (0.816)
Epoch: [12][41/61]	Time 0.21538 (0.26883)	Data 0.00004 (0.05891)	Loss 0.3713 (0.6180)	Acc 0.875 (0.817)
Epoch: [12][42/61]	Time 0.21511 (0.26755)	Data 0.00006 (0.05751)	Loss 0.2948 (0.6103)	Acc 0.875 (0.818)
Epoch: [12][43/61]	Time 0.21513 (0.26633)	Data 0.00004 (0.05617)	Loss 0.9320 (0.6178)	Acc 0.625 (0.814)
Epoch: [12][44/61]	Time 0.21540 (0.26517)	Data 0.00003 (0.05490)	Loss 0.4410 (0.6137)	Acc 0.750 (0.812)
Epoch: [12][45/61]	Time 0.21570 (0.26407)	Data 0.00005 (0.05368)	Loss 1.7029 (0.6380)	Acc 0.500 (0.806)
Epoch: [12][46/61]	Time 0.21564 (0.26302)	Data 0.00005 (0.05251)	Loss 0.2017 (0.6285)	Acc 0.875 (0.807)
Epoch: [12][47/61]	Time 0.21559 (0.26201)	Data 0.00005 (0.05140)	Loss 0.4729 (0.6252)	Acc 0.875 (0.809)
Epoch: [12][48/61]	Time 0.21507 (0.26103)	Data 0.00005 (0.05033)	Loss 0.6251 (0.6252)	Acc 0.875 (0.810)
Epoch: [12][49/61]	Time 0.21530 (0.26010)	Data 0.00006 (0.04930)	Loss 1.2565 (0.6380)	Acc 0.750 (0.809)
Epoch: [12][50/61]	Time 0.21508 (0.25920)	Data 0.00006 (0.04832)	Loss 0.4804 (0.6349)	Acc 0.875 (0.810)
Epoch: [12][51/61]	Time 0.21552 (0.25834)	Data 0.00003 (0.04737)	Loss 2.0170 (0.6620)	Acc 0.625 (0.806)
Epoch: [12][52/61]	Time 0.21583 (0.25753)	Data 0.00005 (0.04646)	Loss 0.4412 (0.6577)	Acc 0.750 (0.805)
Epoch: [12][53/61]	Time 0.21546 (0.25673)	Data 0.00004 (0.04558)	Loss 0.4080 (0.6530)	Acc 0.875 (0.807)
Epoch: [12][54/61]	Time 0.21580 (0.25597)	Data 0.00004 (0.04474)	Loss 0.2586 (0.6457)	Acc 0.875 (0.808)
Epoch: [12][55/61]	Time 0.21597 (0.25525)	Data 0.00006 (0.04393)	Loss 0.1958 (0.6375)	Acc 0.875 (0.809)
Epoch: [12][56/61]	Time 0.21514 (0.25453)	Data 0.00007 (0.04314)	Loss 0.5920 (0.6367)	Acc 0.875 (0.810)
Epoch: [12][57/61]	Time 0.21578 (0.25385)	Data 0.00006 (0.04239)	Loss 0.7063 (0.6380)	Acc 0.750 (0.809)
Epoch: [12][58/61]	Time 0.21530 (0.25319)	Data 0.00005 (0.04166)	Loss 0.2321 (0.6310)	Acc 0.875 (0.810)
Epoch: [12][59/61]	Time 0.21488 (0.25254)	Data 0.00006 (0.04095)	Loss 1.3332 (0.6429)	Acc 0.750 (0.809)
Epoch: [12][60/61]	Time 0.21633 (0.25193)	Data 0.00005 (0.04027)	Loss 1.2518 (0.6530)	Acc 0.625 (0.806)
Epoch: [12][61/61]	Time 0.07160 (0.24898)	Data 0.00003 (0.03961)	Loss 0.2470 (0.6513)	Acc 1.000 (0.807)
     Valid acc: 0.8070539419087137  (0.8008298755186722, ep: 11)
train at epoch 13
Epoch: [13][0/132]	 lr: 0.00100	Time 3.766 (3.766)	Data 2.928 (2.928)	Loss 0.8361 (0.8361)	Acc 0.625 (0.625)	Precision 0.455(0.455)	Recall 0.455(0.455)
Epoch: [13][10/132]	 lr: 0.00100	Time 0.812 (1.079)	Data 0.000 (0.266)	Loss 0.5625 (0.9838)	Acc 0.875 (0.659)	Precision 0.929(0.554)	Recall 0.929(0.536)
Epoch: [13][20/132]	 lr: 0.00100	Time 0.931 (0.999)	Data 0.000 (0.140)	Loss 1.4203 (0.9137)	Acc 0.500 (0.690)	Precision 0.400(0.580)	Recall 0.400(0.557)
Epoch: [13][30/132]	 lr: 0.00100	Time 0.933 (0.978)	Data 0.000 (0.095)	Loss 0.3956 (0.8929)	Acc 0.875 (0.710)	Precision 0.778(0.608)	Recall 0.778(0.581)
Epoch: [13][40/132]	 lr: 0.00100	Time 0.927 (0.967)	Data 0.000 (0.072)	Loss 0.7696 (0.8945)	Acc 0.875 (0.713)	Precision 0.778(0.609)	Recall 0.778(0.582)
Epoch: [13][50/132]	 lr: 0.00100	Time 0.933 (0.960)	Data 0.000 (0.058)	Loss 0.7259 (0.9008)	Acc 0.750 (0.718)	Precision 0.556(0.611)	Recall 0.556(0.585)
Epoch: [13][60/132]	 lr: 0.00100	Time 0.932 (0.955)	Data 0.000 (0.048)	Loss 0.9998 (0.8826)	Acc 0.750 (0.727)	Precision 0.750(0.620)	Recall 0.750(0.596)
Epoch: [13][70/132]	 lr: 0.00100	Time 0.933 (0.952)	Data 0.000 (0.041)	Loss 0.4977 (0.8795)	Acc 0.875 (0.731)	Precision 0.778(0.623)	Recall 0.778(0.602)
Epoch: [13][80/132]	 lr: 0.00100	Time 0.933 (0.950)	Data 0.000 (0.036)	Loss 0.9322 (0.8736)	Acc 0.750 (0.735)	Precision 0.600(0.627)	Recall 0.600(0.606)
Epoch: [13][90/132]	 lr: 0.00100	Time 0.933 (0.948)	Data 0.000 (0.032)	Loss 1.1060 (0.8883)	Acc 0.625 (0.727)	Precision 0.500(0.618)	Recall 0.450(0.596)
Epoch: [13][100/132]	 lr: 0.00100	Time 0.932 (0.947)	Data 0.000 (0.029)	Loss 1.6336 (0.8806)	Acc 0.500 (0.730)	Precision 0.400(0.623)	Recall 0.400(0.601)
Epoch: [13][110/132]	 lr: 0.00100	Time 0.934 (0.946)	Data 0.000 (0.027)	Loss 0.6749 (0.8877)	Acc 0.750 (0.725)	Precision 0.750(0.620)	Recall 0.625(0.597)
Epoch: [13][120/132]	 lr: 0.00100	Time 0.934 (0.945)	Data 0.000 (0.024)	Loss 0.9618 (0.8884)	Acc 0.750 (0.725)	Precision 0.625(0.618)	Recall 0.562(0.595)
Epoch: [13][130/132]	 lr: 0.00100	Time 0.933 (0.944)	Data 0.000 (0.022)	Loss 1.1588 (0.8873)	Acc 0.500 (0.723)	Precision 0.444(0.617)	Recall 0.444(0.593)
validation at epoch 13
